{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9d6c819",
      "metadata": {
        "id": "f9d6c819",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seisbench/seisbench/blob/additional_example_workflows/examples/03a_training_phasenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241827c2",
      "metadata": {
        "id": "241827c2",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "![image](https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c95f074",
      "metadata": {
        "id": "5c95f074",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*This code is necessary on colab to install SeisBench. If SeisBench is already installed on your machine, you can skip this.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "a13e49f1",
      "metadata": {
        "id": "a13e49f1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# !pip install seisbench"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "960e4590",
      "metadata": {
        "id": "960e4590",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*This cell is required to circumvent an issue with colab and obspy. For details, check this issue in the obspy documentation: https://github.com/obspy/obspy/issues/2547*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "a77bdb48",
      "metadata": {
        "id": "a77bdb48",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     import obspy\n",
        "#     obspy.read()\n",
        "# except TypeError:\n",
        "#     # Needs to restart the runtime once, because obspy only works properly after restart.\n",
        "#     print('Stopping RUNTIME. If you run this code for the first time, this is expected. Colaboratory will restart automatically. Please run again.')\n",
        "#     exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd83fdd",
      "metadata": {
        "id": "5dd83fdd",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "As training data we use the ETHZ dataset. Note that we set the sampling rate to 100 Hz to ensure that all examples are consistent in terms of sampling rate. We split the data into training, development and test sets according to the splits provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "32c521b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seisbench.models as sbm\n",
        "import seisbench.generate as sbg\n",
        "import numpy as np\n",
        "import seisbench.data as sbd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from obspy.clients.fdsn import Client\n",
        "from obspy import UTCDateTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6149af14",
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import load_config\n",
        "cfg = load_config('Kaki-cfg.yml')\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbbf8d17",
      "metadata": {
        "id": "dbbf8d17",
        "outputId": "fa32a588-a9be-4c83-e097-98cd8518c1f9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "data = sbd.WaveformDataset(cfg.path.dataset, sampling_rate=100)\n",
        "train, dev, test = data.train_dev_test()\n",
        "print(train, dev, test, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "c85fd78e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [key for key in data.metadata.keys() if 'arrival_sample' in key]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d876fa35",
      "metadata": {
        "id": "d876fa35",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Generation pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c037b8",
      "metadata": {
        "id": "27c037b8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The ETHZ dataset contains detailed labels for the phases. However, for this example we only want to differentiate between P and S picks. Therefore, we define a dictionary mapping the detailed picks to their phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "930b8ed6",
      "metadata": {
        "id": "930b8ed6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "phase_dict = {\n",
        "    \"trace_p_arrival_sample\": \"P\",\n",
        "    \"trace_pP_arrival_sample\": \"P\",\n",
        "    \"trace_P_arrival_sample\": \"P\",\n",
        "    \"trace_P1_arrival_sample\": \"P\",\n",
        "    \"trace_Pg_arrival_sample\": \"P\",\n",
        "    \"trace_Pn_arrival_sample\": \"P\",\n",
        "    \"trace_PmP_arrival_sample\": \"P\",\n",
        "    \"trace_pwP_arrival_sample\": \"P\",\n",
        "    \"trace_pwPm_arrival_sample\": \"P\",\n",
        "    \"trace_s_arrival_sample\": \"S\",\n",
        "    \"trace_S_arrival_sample\": \"S\",\n",
        "    \"trace_S1_arrival_sample\": \"S\",\n",
        "    \"trace_Sg_arrival_sample\": \"S\",\n",
        "    \"trace_SmS_arrival_sample\": \"S\",\n",
        "    \"trace_Sn_arrival_sample\": \"S\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc7cfc9",
      "metadata": {
        "id": "8bc7cfc9",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now we define two generators with identical augmentations, one for training, one for validation. The augmentations are:\n",
        "1. Selection of a (long) window around a pick. This way, we ensure that out data always contains a pick.\n",
        "1. Selection of a random window with 3001 samples, the input length of PhaseNet.\n",
        "1. A normalization, consisting of demeaning and amplitude normalization.\n",
        "1. A change of datatype to float32, as this is expected by the pytorch model.\n",
        "1. A probabilistic label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "cce52377",
      "metadata": {
        "id": "cce52377",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "augmentations = [\n",
        "    sbg.WindowAroundSample(\n",
        "        list(phase_dict.keys()),\n",
        "        samples_before=3000, windowlen=6000,\n",
        "        selection=\"random\", strategy=\"variable\"),\n",
        "    sbg.RandomWindow(\n",
        "        windowlen=3001, strategy=\"pad\"),\n",
        "    sbg.Normalize(\n",
        "        demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
        "    sbg.ChangeDtype(\n",
        "        np.float32),\n",
        "    sbg.ProbabilisticLabeller(\n",
        "        label_columns=phase_dict, sigma=30, dim=0)\n",
        "]\n",
        "\n",
        "dev_generator = sbg.GenericGenerator(dev)\n",
        "dev_generator.add_augmentations(augmentations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005455e0",
      "metadata": {
        "id": "005455e0",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "SeisBench generators are pytorch datasets. Therefore, we can pass them to pytorch data loaders. These will automatically take care of parallel loading and batching. Here we create one loader for training and one for validation. We choose a batch size of 256 samples. This batch size should fit on most hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9024bf3c",
      "metadata": {
        "id": "9024bf3c",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now we got all components for training the model. What we still need to do is define the optimizer and the loss, and write the training and validation loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ff561a",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = sbm.PhaseNet.load(cfg.path.dl_model, version_str=cfg.training.version_str)\n",
        "model_org = sbm.PhaseNet.from_pretrained('original')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a43e14",
      "metadata": {
        "id": "93a43e14",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Evaluating the model\n",
        "\n",
        "Not that we trained the model, we can evaluate it. First, we'll check how the model does on an example from the development set. Note that the model will most likely not be fully trained after only five epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "768fc23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visual_eval(data_X, data_Y, model1, model2):\n",
        "    with torch.no_grad():\n",
        "        pred1 = model1(torch.tensor(\n",
        "            data_X,\n",
        "            device=model1.device).unsqueeze(0)\n",
        "        )\n",
        "        pred1 = pred1[0].cpu().numpy()\n",
        "        #\n",
        "        pred2 = model2(torch.tensor(\n",
        "            data_X,\n",
        "            device=model2.device).unsqueeze(0)\n",
        "        )\n",
        "        pred2 = pred2[0].cpu().numpy()\n",
        "    #\n",
        "    fig, axs = plt.subplots(\n",
        "        4, 1,\n",
        "        figsize=(15, 7),\n",
        "        sharex=True,\n",
        "        gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1, 1, 1]}\n",
        "    )\n",
        "    ax1, ax2, ax3, ax4 = axs\n",
        "    #\n",
        "    ax1.plot(data_X.T)\n",
        "    ax1.set_ylabel('Waveform')\n",
        "    #\n",
        "    ax2.plot(data_Y.T)\n",
        "    ax2.set_ylabel('Manual')\n",
        "    #\n",
        "    ax3.plot(pred1.T)\n",
        "    ax3.set_ylabel('Mine')\n",
        "    #\n",
        "    pred2 = pred2[[1, 2, 0], :]\n",
        "    ax4.plot(pred2.T)\n",
        "    ax4.set_ylabel('Origin')\n",
        "    #\n",
        "    ax1.set_yticks([0, 0.5, 1])\n",
        "    for ax in [ax2, ax3, ax4]:\n",
        "        ax.set_yticks([0.5, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd81efe1",
      "metadata": {},
      "outputs": [],
      "source": [
        "rand_num = np.random.randint(len(dev_generator))\n",
        "sample = dev_generator[rand_num]\n",
        "visual_eval(data_X=sample[\"X\"],\n",
        "            data_Y=sample[\"y\"],\n",
        "            model1=model,\n",
        "            model2=model_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "4d14e180",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = sample[\"y\"]\n",
        "p, s, n = y.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "19012f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def numeric_eval(data_X, data_Y, model1, model2):\n",
        "    with torch.no_grad():\n",
        "        pred1 = model1(torch.tensor(\n",
        "            data_X,\n",
        "            device=model1.device).unsqueeze(0)\n",
        "        )\n",
        "        pred1 = pred1[0].cpu().numpy()\n",
        "        #\n",
        "        pred2 = model2(torch.tensor(\n",
        "            data_X,\n",
        "            device=model2.device).unsqueeze(0)\n",
        "        )\n",
        "        pred2 = pred2[0].cpu().numpy()\n",
        "        pred2 = pred2[[1, 2, 0], :]\n",
        "    p0, s0, n0 = data_Y.argmax(axis=1)\n",
        "    p1, s1, n1 = pred1.argmax(axis=1)\n",
        "    p2, s2, n2 = pred2.argmax(axis=1)\n",
        "    return {'p': [p0, p1, p2], 's': [s0, s1, s2]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84e6902",
      "metadata": {},
      "outputs": [],
      "source": [
        "lst_stats = []\n",
        "for ii in range(len(dev_generator)):\n",
        "    sample = dev_generator[ii]\n",
        "    stat = numeric_eval(\n",
        "        data_X=sample[\"X\"],\n",
        "        data_Y=sample[\"y\"],\n",
        "        model1=model,\n",
        "        model2=model_org)\n",
        "    lst_stats.append(stat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e73665",
      "metadata": {},
      "outputs": [],
      "source": [
        "for phase in ['p', 's']:\n",
        "    arr = []\n",
        "    for stat in lst_stats:\n",
        "        arr.append(stat[phase])\n",
        "    arr = np.array(arr)\n",
        "    m1 = arr[:, 0] - arr[:, 1]\n",
        "    m2 = arr[:, 0] - arr[:, 2]\n",
        "    #\n",
        "    sps = 100\n",
        "    m1 = m1 / sps\n",
        "    m2 = m2 / sps\n",
        "\n",
        "    lim = 2 # seconds\n",
        "    step = lim/10\n",
        "    m1 = m1[np.abs(m1)<=lim]\n",
        "    m2 = m2[np.abs(m2)<=lim]\n",
        "    bins = np.arange(-lim, lim, step) + step/2\n",
        "    plt.hist(m1, bins=bins, edgecolor='k')\n",
        "    plt.title('m1'+phase)\n",
        "    plt.show()\n",
        "    plt.hist(m2, bins=bins, edgecolor='k')\n",
        "    plt.title('m2'+phase)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d48017ff",
      "metadata": {
        "id": "d48017ff",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "As a second option, we'll directly apply our model to an obspy waveform stream using the `annotate` function. For this, we are downloading waveforms through FDSN and annotating them. Note that you could use the `classify` function in a similar fashion.\n",
        "\n",
        "As we trained the model on Swiss data, we use an example event from Switzerland. Note that we deliberately chose a rather easy example, as the model is not fully trained after the low number of epochs. The exact performance of the model will vary depending, because the model training and initialization involves random aspects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067fcf5d",
      "metadata": {
        "id": "067fcf5d",
        "outputId": "1ab35754-947e-4e28-f637-185a35054bb9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "client = Client(\"ETH\")\n",
        "\n",
        "t = UTCDateTime(\"2019-11-04T00:59:46.419800Z\")\n",
        "stream = client.get_waveforms(network=\"CH\", station=\"EMING\", location=\"*\", channel=\"HH?\", starttime=t-30, endtime=t+50)\n",
        "\n",
        "annotations = model.annotate(stream)\n",
        "annotations_org = model_org.annotate(stream)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(15, 7))\n",
        "axs = fig.subplots(3, 1, sharex=True, gridspec_kw={'hspace': 0})\n",
        "\n",
        "offset = annotations[0].stats.starttime - stream[0].stats.starttime\n",
        "offset_org = annotations_org[0].stats.starttime - stream[0].stats.starttime\n",
        "\n",
        "for i in range(3):\n",
        "    axs[0].plot(stream[i].times(), stream[i].data, label=stream[i].stats.channel)\n",
        "    if annotations[i].stats.channel[-1] != \"N\":  # Do not plot noise curve\n",
        "        axs[1].plot(annotations[i].times() + offset, annotations[i].data, label=annotations[i].stats.channel)\n",
        "    if annotations_org[i].stats.channel[-1] != \"N\":  # Do not plot noise curve\n",
        "        axs[2].plot(annotations_org[i].times() + offset_org,\n",
        "                    annotations_org[i].data,\n",
        "                    label=annotations_org[i].stats.channel)\n",
        "\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "axs[2].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd0ef68",
      "metadata": {
        "id": "bdd0ef68",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Remarks\n",
        "\n",
        "As discussed in the data basics tutorial, loading a SeisBench dataset only means loading the metadata into memory. The waveforms are only loaded once they are requested to save memory. By default, waveforms are **not** cached in memory. For training, this means that the data needs to be read from the file in every epoch again. Depending on your hardware, this will take a lot of time. To solve this issue, you can set the `cache` option, when creating the dataset. Then, all you have to do is call `preload_waveforms` and the data will be loaded into memory and automatically cached. For most practical applications, this option is recommended."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
