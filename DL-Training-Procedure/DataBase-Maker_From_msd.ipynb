{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afc9885",
   "metadata": {
    "id": "5afc9885",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating a dataset\n",
    "\n",
    "This tutorial provides a short example on how to create a SeisBench dataset. Datasets can be created from any event catalog and waveform collection. For this example, we download an event catalog for Switzerland through FDSN. We will then download the associated waveforms through FDSN as well. We use built-in SeisBench functions for writing out the dataset in SeisBench format. In this example notebook we aim for an easy example outlining the principles of dataset creation. There are a few further considerations, in particular, for converting larger datasets, that we outline at the end.\n",
    "\n",
    "**Note:** Some familiarity with obspy and its FDSN client is helpful for this tutorial, but not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d5a03f",
   "metadata": {
    "id": "a5d5a03f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seisbench.data as sbd\n",
    "import seisbench.util as sbu\n",
    "\n",
    "from pathlib import Path\n",
    "from obspy import read_events\n",
    "from obspy import read\n",
    "import pandas as pd\n",
    "from config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41326bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, root, pattern_path):\n",
    "        self.root = root\n",
    "        self.pattern_path = pattern_path\n",
    "        self.stream = None\n",
    "        self.stats  = None\n",
    "\n",
    "    def read(self, time):\n",
    "        pattern = self.pattern_path.format(time=time)\n",
    "        path = f'{self.root}/{pattern}'\n",
    "        print('Reading Data:', path)\n",
    "        self.stream = read(path)\n",
    "        self.preprocessing_data()\n",
    "        self.stations = list({tr.stats.station for tr in self.stream})\n",
    "\n",
    "    def get_data_related_to_pick(self, pick):\n",
    "        if self.stream is None:\n",
    "            self.read(time=pick.time)\n",
    "        if not pick.waveform_id.station_code in self.stations:\n",
    "            self.read(time=pick.time)\n",
    "        if not pick.time.julday == self.stream[0].stats.starttime.julday:\n",
    "            self.read(time=pick.time)\n",
    "        target_stream = self.stream.select(station=pick.waveform_id.station_code)\n",
    "        return target_stream\n",
    "    \n",
    "    def preprocessing_data(self):\n",
    "        self.sps_check()\n",
    "        self.stream.merge(-1)\n",
    "        self.stream.detrend('constant')\n",
    "        self.stream.merge(method=1, fill_value=0)\n",
    "    \n",
    "    def sps_check(self):\n",
    "        sps = self.stream[0].stats.sampling_rate\n",
    "        assert all(tr.stats.sampling_rate == sps for tr in self.stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d76abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ffffa",
   "metadata": {},
   "source": [
    "#### Loading configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de598521",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config('Kaki-cfg.yml')\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d14a32",
   "metadata": {
    "id": "91d14a32",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The event catalog\n",
    "\n",
    "As a first step, we need an event catalog. Here, we are going to use the catalog provided by ETHZ over FDSN. For demonstration purposes, we only use a short time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d087f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_details = pd.read_csv(cfg.path.network_details, dtype=str)\n",
    "network_details.fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = read_events(cfg.path.catalog)\n",
    "catalog = [ev for ev in catalog if ev.picks != []]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680219df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(catalog), catalog, sep='\\n') # print(catalog.__str__(print_all=True))\n",
    "\n",
    "lst = []\n",
    "for ev in catalog:\n",
    "    for pick in ev.picks:\n",
    "        lst.append(pick.phase_hint)\n",
    "\n",
    "for el in set(lst):\n",
    "    print(el, lst.count(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaef76",
   "metadata": {
    "id": "29aaef76",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the event parameters\n",
    "\n",
    "From the catalog, we extract the event parameters and store them into a dictionary. Here, we only extract a few basic parameters on the source and its magnitude - if available. In addition, we define the split of the dataset into training/development/test partitions. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18becd85",
   "metadata": {
    "id": "18becd85",
    "outputId": "80b15fee-195d-468a-fa9f-e80e368c21fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_event_params(event):\n",
    "    origin = event.preferred_origin()\n",
    "    mag = event.preferred_magnitude()\n",
    "\n",
    "    source_id = str(event.resource_id)\n",
    "\n",
    "    event_params = {\n",
    "        \"source_id\": source_id,\n",
    "        \"source_origin_time\": str(origin.time),\n",
    "        \"source_origin_uncertainty_sec\": origin.time_errors[\"uncertainty\"],\n",
    "        \"source_latitude_deg\": origin.latitude,\n",
    "        \"source_latitude_uncertainty_km\": origin.latitude_errors[\"uncertainty\"],\n",
    "        \"source_longitude_deg\": origin.longitude,\n",
    "        \"source_longitude_uncertainty_km\": origin.longitude_errors[\"uncertainty\"],\n",
    "        \"source_depth_km\": origin.depth / 1e3            if origin.depth else None,\n",
    "        \"source_depth_uncertainty_km\": origin.depth_errors[\"uncertainty\"] / 1e3           if origin.depth else None,\n",
    "    }\n",
    "\n",
    "    if mag is not None:\n",
    "        event_params[\"source_magnitude\"] = mag.mag\n",
    "        event_params[\"source_magnitude_uncertainty\"] = mag.mag_errors[\"uncertainty\"]\n",
    "        event_params[\"source_magnitude_type\"] = mag.magnitude_type\n",
    "        event_params[\"source_magnitude_author\"] = mag.creation_info.agency_id\n",
    "\n",
    "        if str(origin.time) < \"2015-01-07\":\n",
    "            split = \"train\"\n",
    "        elif str(origin.time) < \"2015-01-08\":\n",
    "            split = \"dev\"\n",
    "        else:\n",
    "            split = \"test\"\n",
    "        event_params[\"split\"] = split\n",
    "    return event_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b7f36",
   "metadata": {
    "id": "917b7f36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the trace parameters\n",
    "\n",
    "From each pick, we extract parameters about the trace and store them in a dictionary. Again, we only extract very basic parameters. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0e47c0",
   "metadata": {
    "id": "ef0e47c0",
    "outputId": "cf98a9a5-eed4-4fca-ce2f-ff7cb4f9509d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trace_params(pick):\n",
    "    net = pick.waveform_id.network_code\n",
    "    sta = pick.waveform_id.station_code\n",
    "\n",
    "    trace_params = {\n",
    "        \"station_network_code\": net,\n",
    "        \"station_code\": sta,\n",
    "        \"trace_channel\": pick.waveform_id.channel_code[:2],\n",
    "        \"station_location_code\": pick.waveform_id.location_code,\n",
    "        \"evaluation_mode\": pick.evaluation_mode\n",
    "    }\n",
    "\n",
    "    return trace_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07de633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_arrival_of_pick(pick, arrivals):\n",
    "    '''\n",
    "    Docstring\n",
    "    '''\n",
    "    find_arrival = False\n",
    "    for arrival in arrivals:\n",
    "        if arrival.pick_id == pick.resource_id:\n",
    "            find_arrival = True\n",
    "            break\n",
    "    if not find_arrival:\n",
    "        arrival = False\n",
    "    return arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ff77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# j = 0\n",
    "# for ev in catalog:\n",
    "#     origin = ev.preferred_origin()\n",
    "#     for pick in ev.picks:\n",
    "#         arrival = select_arrival_of_pick(pick=pick, arrivals=origin.arrivals)\n",
    "#         if arrival==False:\n",
    "#             i += 1\n",
    "#             break\n",
    "#         else:\n",
    "#             j += 1\n",
    "#         # print(arrival, pick, '\\n\\n', sep='\\n')\n",
    "# print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d8b959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_params(pick, event):\n",
    "    origin = event.preferred_origin()\n",
    "    arrival = select_arrival_of_pick(pick=pick, arrivals=origin.arrivals)\n",
    "    #\n",
    "    phase_params = arrival.__dict__.copy()\n",
    "    for key in ['resource_id', 'pick_id', 'phase']:\n",
    "        phase_params.pop(key)\n",
    "    phase_params = {f'{key}_{pick.phase_hint}':val for key,val in phase_params.items()}\n",
    "    return phase_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "552d033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev = catalog[0]\n",
    "# pick = ev.picks[1]\n",
    "# get_phase_params(pick, ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc36b0",
   "metadata": {},
   "source": [
    "## Noise Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import Stream, Trace\n",
    "import numpy as np\n",
    "###\n",
    "def tr_noise_padding(tr, stime, etime, std_windows=(2, 2)):\n",
    "    if isinstance(stime, float | int):\n",
    "        stime = tr.stats.starttime - stime\n",
    "    if isinstance(etime, float | int):\n",
    "        etime = tr.stats.endtime + etime\n",
    "    ###\n",
    "    lst_tr = [tr]\n",
    "    sps = tr.stats.sampling_rate\n",
    "    ###\n",
    "    sduration = (tr.stats.starttime - stime) * sps\n",
    "    sduration = int(sduration)\n",
    "    if sduration > 0:\n",
    "        tr_std_s = tr.slice(endtime=tr.stats.starttime+std_windows[0])\n",
    "        std_s = tr_std_s.std()\n",
    "        snoise = np.random.normal(loc=0.0, scale=std_s, size=sduration)\n",
    "        strn = Trace(snoise)\n",
    "        strn.id = tr.id\n",
    "        strn.stats.sampling_rate = sps\n",
    "        strn.stats.starttime = tr.stats.starttime\n",
    "        strn.stats.starttime -= (strn.stats.npts/sps)\n",
    "        lst_tr.append(strn)\n",
    "    ###\n",
    "    eduration = (etime - tr.stats.endtime) * sps\n",
    "    eduration = int(eduration)\n",
    "    if eduration > 0:\n",
    "        tr_std_e = tr.slice(starttime=tr.stats.endtime-std_windows[1])\n",
    "        std_e = tr_std_e.std()\n",
    "        enoise = np.random.normal(loc=0.0, scale=std_e, size=eduration)\n",
    "        etrn = Trace(enoise)\n",
    "        etrn.id = tr.id\n",
    "        etrn.stats.sampling_rate = sps\n",
    "        etrn.stats.starttime = tr.stats.endtime + 1/sps\n",
    "        lst_tr.append(etrn)\n",
    "    ###\n",
    "    st = Stream(lst_tr)\n",
    "    st.merge(-1)\n",
    "    if st.get_gaps() == []:\n",
    "        return st[0]\n",
    "    else:\n",
    "        print('There was a problem in noise-padding!')\n",
    "        print(st)\n",
    "        st.print_gaps()\n",
    "        return None\n",
    "\n",
    "def st_noise_padding(st, stime, etime, std_windows=(2, 2)):\n",
    "    st.merge(-1)\n",
    "    st.detrend('constant')\n",
    "    st.merge(fill_value=0)\n",
    "    st_new = Stream()\n",
    "    for tr in st:\n",
    "        st_new += tr_noise_padding(\n",
    "            tr=tr, stime=stime, etime=etime, std_windows=std_windows\n",
    "        )\n",
    "    return st_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab655daa",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d2a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reversing_dictionary(dictionary):\n",
    "    return {v:k for k, v in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be38a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(st, pick, before, after):\n",
    "    st_slice = st.slice(\n",
    "        starttime=pick.time-before,\n",
    "        endtime=pick.time+after,\n",
    "        nearest_sample=True)\n",
    "    st_slice.trim(\n",
    "        starttime=pick.time-before,\n",
    "        endtime=pick.time+after,\n",
    "        pad=True,\n",
    "        nearest_sample=True,\n",
    "        fill_value=0)\n",
    "    return st_slice\n",
    "\n",
    "def trim_data(st, pick, before, after):\n",
    "    st.trim(\n",
    "        starttime=pick.time-before,\n",
    "        endtime=pick.time+after,\n",
    "        pad=True,\n",
    "        nearest_sample=True,\n",
    "        fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c45894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(st):\n",
    "    st.merge(-1)\n",
    "    st.detrend('constant')\n",
    "    st.merge(fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45494c",
   "metadata": {
    "id": "be45494c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Writing to SeisBench format\n",
    "\n",
    "Now, we can combine all the above functions together to write a dataset in SeisBench format. For this, we first need to define the path. For this example, we are using the current working directory. A dataset consists of 2 components:\n",
    " - a metadata file, always called `metadata.csv`, which contains all the associated properties of the waveform examples (e.g. trace parameters, source parameters etc.).\n",
    " - a waveforms file, always called `waveforms.hdf5`, containing the raw waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0d5f8",
   "metadata": {
    "id": "30d0d5f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_path = Path(cfg.path.dataset)\n",
    "metadata_path = base_path / \"metadata.csv\"\n",
    "waveforms_path = base_path / \"waveforms.hdf5\"\n",
    "print(metadata_path, waveforms_path, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69c41a",
   "metadata": {
    "id": "ea69c41a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To write the dataset, we use the `WaveformDataWriter` provided by SeisBench. The writer should always be used as a context manager, i.e., using the `with` statement, as shown below. This is to ensure files are properly clsoed after writing and teardown and cleanup operations are always called when exiting the context manager.\n",
    "\n",
    "First, we need to set the data format for our dataset. We do this by assigning a dictionary to the `writer.data_format` group.\n",
    "\n",
    "Next, we iterate over all event and all picks in the events. Using the functions above, we generate the event and trace metadata and download the waveforms. We then convert the waveforms to a numpy array using the function `stream_to_array` provided in `seisbench.util`.\n",
    "\n",
    "As a last step, we hand the event metadata and the waveforms as numpy array over to the writer using `add_trace`. The writer then automatically takes care of writing out the data in the correct format. It also takes care of performance optimisations that we outline in the further considerations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eba60659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_picks(picks, station_name, without_amplitued=True):\n",
    "    picks = [pick for pick in picks\n",
    "             if pick.waveform_id.station_code==station_name]\n",
    "    picks = sorted(picks,\n",
    "                   key= lambda p: p.time)\n",
    "    if without_amplitued:\n",
    "        picks = [pick for pick in picks\n",
    "                 if not pick.phase_hint.startswith('AM')]\n",
    "    return picks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "533508ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picks_time_difference(picks):\n",
    "    picks_time = [pick.time for pick in picks]\n",
    "    picks_time = sorted(picks_time)\n",
    "    picks_difftime = [time-picks_time[0] for time in picks_time]\n",
    "    return picks_difftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "041c98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_equal_sps(stream):\n",
    "    sps = stream[0].stats.sampling_rate\n",
    "    assert all(tr.stats.sampling_rate == sps for tr in stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4236107",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_waveforms = get_data(cfg.path.stream_root, cfg.path.stream_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec34f6",
   "metadata": {
    "id": "dbec34f6",
    "outputId": "d50fa6fd-ed8a-4932-c9db-c1fad4246dd1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over events and picks, write to SeisBench format\n",
    "with sbd.WaveformDataWriter(metadata_path, waveforms_path) as writer:\n",
    "\n",
    "    # Define data format\n",
    "    writer.data_format = {\n",
    "        \"dimension_order\": \"CW\",\n",
    "        \"component_order\": \"ZNE\",\n",
    "        \"measurement\": \"velocity\",\n",
    "        \"unit\": \"counts\",\n",
    "        \"instrument_response\": \"not restituted\",\n",
    "    }\n",
    "    n_all = len(catalog)\n",
    "    for index, event in enumerate(catalog):\n",
    "        # if index < 1050:\n",
    "        #     continue\n",
    "        if index % 500 == 0:\n",
    "            print(f'{index} of {n_all} ({index/n_all*100:.2f}%)')\n",
    "        # if index == 200:\n",
    "        #     break\n",
    "\n",
    "        event_params = get_event_params(event)\n",
    "        stations_in_event = {pick.waveform_id.station_code for pick in event.picks}\n",
    "        for station_name in stations_in_event:\n",
    "            picks = select_picks(picks=event.picks,\n",
    "                                 station_name=station_name)\n",
    "            if picks == []:\n",
    "                continue\n",
    "            ###\n",
    "            phase_params = {}\n",
    "            for pick in picks:\n",
    "                param = get_phase_params(pick, event)\n",
    "                phase_params.update(param)\n",
    "            ###\n",
    "            time_diff = get_picks_time_difference(picks)\n",
    "            if max(time_diff) >= 60:\n",
    "                print(f'losing pick, maximume is: {max(time_diff)}')\n",
    "            ###\n",
    "            pick = picks[0]\n",
    "            trace_params = get_trace_params(pick)\n",
    "            waveforms = get_waveforms.get_data_related_to_pick(pick=pick)\n",
    "            waveforms = slice_data(waveforms, pick,\n",
    "                                   before=cfg.cut_time.before, \n",
    "                                   after=cfg.cut_time.after)\n",
    "            waveforms = st_noise_padding(\n",
    "                st=waveforms,\n",
    "                stime=cfg.noisepad.before,\n",
    "                etime=cfg.noisepad.after,\n",
    "                std_windows=(cfg.noisepad.std_start, cfg.noisepad.std_end)\n",
    "                )\n",
    "            ### Check remaining data\n",
    "            if len(waveforms) == 0:\n",
    "                # No waveform data available\n",
    "                print('There is No WaveForms After Slicing!!!')\n",
    "                continue\n",
    "            ###\n",
    "            sampling_rate = waveforms[0].stats.sampling_rate\n",
    "            # Check that the traces have the same sampling rate\n",
    "            checking_equal_sps(stream=waveforms)\n",
    "\n",
    "            actual_t_start, data, _ = sbu.stream_to_array(\n",
    "                waveforms,\n",
    "                component_order=writer.data_format[\"component_order\"],\n",
    "            )\n",
    "            trace_params[\"trace_sampling_rate_hz\"] = sampling_rate\n",
    "            trace_params[\"trace_start_time\"] = str(actual_t_start)\n",
    "\n",
    "            for pick in picks:\n",
    "                sample = (pick.time - actual_t_start) * sampling_rate\n",
    "                trace_params[f\"trace_{pick.phase_hint}_arrival_sample\"] = int(sample)\n",
    "                trace_params[f\"trace_{pick.phase_hint}_status\"] = pick.evaluation_mode\n",
    "\n",
    "            writer.add_trace({**event_params, **trace_params, **phase_params}, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2930fb",
   "metadata": {
    "id": "1c2930fb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Considerations for converting datasets\n",
    "\n",
    "As outlined above, this tutorial provides a very minimal example on converting a dataset. Here we outline additional consideration that should be taken into account when preparing a dataset.\n",
    "\n",
    "- **Grouping picks**: In this example, we created one trace for each pick. Naturally, traces will overlap if multiple picks, e.g., P and S phases, are available for an event at a station. For an example implementation of this grouping operation, have a look [here](https://github.com/seisbench/seisbench/blob/df94dcd86ce66d6a2ee2bd00da3857259fe579bd/seisbench/data/ethz.py#L109) and in the subsequent lines.\n",
    "- **Adding station information**: In this example, we added no station information except its name. In practice, it will often be helpful for users to incorporate, for example, the location of the station. We skipped this step here, because it requires loading station inventories through FDSN. For an example implementation, have a look [here](https://github.com/seisbench/seisbench/blob/df94dcd86ce66d6a2ee2bd00da3857259fe579bd/seisbench/data/ethz.py#L315).\n",
    "- **Memory requirements**: Internally, the `WaveformDataWriter` writes out the the waveforms continuously in blocks (see point below), but keeps all metadata in memory until the dataset is complete. For very large datasets (or very detailed metadata) this can result in several gigabytes of memory consumption. If you are writing such datasets, make sure the available memory on your machine is sufficient.\n",
    "- **Waveform blocks**: Instead of writing each waveform separately, waveforms are written out in blocks. This massively improves IO performance. Have a look at [the documentation](https://seisbench.readthedocs.io/en/stable/pages/data_format.html#traces-blocks) for details on the strategy. We expect that in nearly all cases using the default setting will be a good choice.\n",
    "- **FDSN considerations**: When converting very large datasets, the performance might be limited by the performance of the FDSN webservice. Unfortunately, downloading lots of short waveforms (as required for many machine learning applications) does not seem to be the most favorable use case for FDSN. This leads to rather slow performance when naively downloading the waveforms as outlined above. Instead, it is often helpful to issue [bulk requests](https://docs.obspy.org/master/packages/autogen/obspy.clients.fdsn.client.Client.get_waveforms_bulk.html). In addition, it might be a good choice to first download the waveforms and cache them locally, for example, in .mseed format, and then convert them to SeisBench.\n",
    "\n",
    "For further details on the data format, check out [the data format specification in the SeisBench documentation](https://seisbench.readthedocs.io/en/stable/pages/data_format.html#traces-blocks)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
