{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afc9885",
   "metadata": {
    "id": "5afc9885",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating a dataset\n",
    "\n",
    "This tutorial provides a short example on how to create a SeisBench dataset. Datasets can be created from any event catalog and waveform collection. For this example, we download an event catalog for Switzerland through FDSN. We will then download the associated waveforms through FDSN as well. We use built-in SeisBench functions for writing out the dataset in SeisBench format. In this example notebook we aim for an easy example outlining the principles of dataset creation. There are a few further considerations, in particular, for converting larger datasets, that we outline at the end.\n",
    "\n",
    "**Note:** Some familiarity with obspy and its FDSN client is helpful for this tutorial, but not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5a03f",
   "metadata": {
    "id": "a5d5a03f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seisbench.data as sbd\n",
    "import seisbench.util as sbu\n",
    "from pathlib import Path\n",
    "from obspy import read_events\n",
    "from obspy import read\n",
    "from obspy import Stream\n",
    "import pandas as pd\n",
    "from config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d76abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ffffa",
   "metadata": {},
   "source": [
    "#### Loading configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de598521",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config('/home/ekarkooti/ikahbasi-PhD/Dataset/Kaki/Kaki-cfg.yml')\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d14a32",
   "metadata": {
    "id": "91d14a32",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The event catalog\n",
    "\n",
    "As a first step, we need an event catalog. Here, we are going to use the catalog provided by ETHZ over FDSN. For demonstration purposes, we only use a short time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_details = pd.read_csv(cfg.path.network_details, dtype=str)\n",
    "network_details.fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = read_events(cfg.path.catalog)\n",
    "catalog = [ev for ev in catalog if ev.picks != []]\n",
    "catalog = catalog[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680219df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(catalog), catalog, sep='\\n') # print(catalog.__str__(print_all=True))\n",
    "\n",
    "lst = []\n",
    "for ev in catalog:\n",
    "    for pick in ev.picks:\n",
    "        lst.append(pick.phase_hint)\n",
    "\n",
    "for el in set(lst):\n",
    "    print(el, lst.count(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaef76",
   "metadata": {
    "id": "29aaef76",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the event parameters\n",
    "\n",
    "From the catalog, we extract the event parameters and store them into a dictionary. Here, we only extract a few basic parameters on the source and its magnitude - if available. In addition, we define the split of the dataset into training/development/test partitions. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18becd85",
   "metadata": {
    "id": "18becd85",
    "outputId": "80b15fee-195d-468a-fa9f-e80e368c21fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_event_params(event):\n",
    "    origin = event.preferred_origin()\n",
    "    mag = event.preferred_magnitude()\n",
    "\n",
    "    source_id = str(event.resource_id)\n",
    "\n",
    "    event_params = {\n",
    "        \"source_id\": source_id,\n",
    "        \"source_origin_time\": str(origin.time),\n",
    "        \"source_origin_uncertainty_sec\": origin.time_errors[\"uncertainty\"],\n",
    "        \"source_latitude_deg\": origin.latitude,\n",
    "        \"source_latitude_uncertainty_km\": origin.latitude_errors[\"uncertainty\"],\n",
    "        \"source_longitude_deg\": origin.longitude,\n",
    "        \"source_longitude_uncertainty_km\": origin.longitude_errors[\"uncertainty\"],\n",
    "        \"source_depth_km\": origin.depth / 1e3            if origin.depth else None,\n",
    "        \"source_depth_uncertainty_km\": origin.depth_errors[\"uncertainty\"] / 1e3           if origin.depth else None,\n",
    "    }\n",
    "\n",
    "    if mag is not None:\n",
    "        event_params[\"source_magnitude\"] = mag.mag\n",
    "        event_params[\"source_magnitude_uncertainty\"] = mag.mag_errors[\"uncertainty\"]\n",
    "        event_params[\"source_magnitude_type\"] = mag.magnitude_type\n",
    "        event_params[\"source_magnitude_author\"] = mag.creation_info.agency_id\n",
    "\n",
    "        if str(origin.time) < \"2015-01-07\":\n",
    "            split = \"train\"\n",
    "        elif str(origin.time) < \"2015-01-08\":\n",
    "            split = \"dev\"\n",
    "        else:\n",
    "            split = \"test\"\n",
    "        event_params[\"split\"] = split\n",
    "    return event_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b7f36",
   "metadata": {
    "id": "917b7f36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the trace parameters\n",
    "\n",
    "From each pick, we extract parameters about the trace and store them in a dictionary. Again, we only extract very basic parameters. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e47c0",
   "metadata": {
    "id": "ef0e47c0",
    "outputId": "cf98a9a5-eed4-4fca-ce2f-ff7cb4f9509d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trace_params(pick):\n",
    "    net = pick.waveform_id.network_code\n",
    "    sta = pick.waveform_id.station_code\n",
    "\n",
    "    trace_params = {\n",
    "        \"station_network_code\": net,\n",
    "        \"station_code\": sta,\n",
    "        \"trace_channel\": pick.waveform_id.channel_code[:2],\n",
    "        \"station_location_code\": pick.waveform_id.location_code,\n",
    "    }\n",
    "\n",
    "    return trace_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564335e",
   "metadata": {
    "id": "b564335e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Downloading the waveforms\n",
    "\n",
    "As a last step, we need to access the waveforms. As for the catalog, we download the waveforms from ETHZ via FDSN. Note that not for all picks we can expect to have waveforms available through FDSN, so we just return empty streams if no data is available. We visualize one example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab655daa",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rename_stream_mapper(obspy_stream, networks, stations, locations, channels):\n",
    "#     for tr in obspy_stream:\n",
    "#         tr.stats.network = networks[tr.stats.network]\n",
    "#         tr.stats.station = stations[tr.stats.station]\n",
    "#         tr.stats.location = locations[tr.stats.location]\n",
    "#         tr.stats.channel = channels[tr.stats.channel]\n",
    "\n",
    "def reversing_dictionary(dictionary):\n",
    "    return {v:k for k, v in dictionary.items()}\n",
    "\n",
    "networks = {'': ''}\n",
    "stations = {'6210': 'HANA',# 6210-Hana-shour  \n",
    "            '6249': 'SORM',# 6249-Sarmak ??? SORM\n",
    "            '6260': 'CHAH',# 6260-Chahgah     \n",
    "            '6270': 'LAVA',# 6270-Lavar\n",
    "            '6215': 'JASH',# 6215-Jashk       \n",
    "            '6251': 'SANA',# 6251-Sana              \n",
    "            '6261': 'SHON',# 6261-Shonbeh     \n",
    "            '6289': 'ABDA',# 6289-Abdan\n",
    "            '6218': 'KERD',# 6218-Kerdelan    \n",
    "            '6252': 'BONY',# 6252-Bonyad            \n",
    "            '6266': 'KARD',# 6266-Kardaneh\n",
    "            '6219': 'DOHO',# 6219-Dohouk      \n",
    "            '6255': 'BOBM',# 6255-Babmonir          \n",
    "            '6268': 'ESLA',# 6268-Eslam-Abad\n",
    "            '6226': 'BAGH',# 6226-Baghan      \n",
    "            '6259': 'GENK',# 6259-Genkhak-Sheikhha  \n",
    "            '6270': 'DARV',# 6270-drveshei\n",
    "            }\n",
    "\n",
    "locations = {'': ''}\n",
    "\n",
    "channels = {'HHZ': 'HHZ',\n",
    "            'HHN': 'HHN',\n",
    "            'HHE': 'HHE'}\n",
    "\n",
    "stationsr = reversing_dictionary(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7e9d2",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from obspy import UTCDateTime as utc\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(cfg.path.network_details, dtype=str)\n",
    "\n",
    "def rename_stream_mapper(stream, dataframe):\n",
    "    for tr in stream:\n",
    "        df = dataframe[dataframe.eq(tr.stats.station).any(axis=1)]\n",
    "        for indx, row in df.iterrows():\n",
    "            cond1 = utc(row.start) <= tr.stats.starttime\n",
    "            cond2 = tr.stats.endtime <= utc(row.end)\n",
    "            if cond1 and cond2:\n",
    "                tr.stats.network = row.network\n",
    "                tr.stats.station = row.station\n",
    "                tr.stats.location = row.location\n",
    "                tr.stats.channel = row.channel + tr.stats.channel[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_station_name(dataframe, sensor_or_station, time):\n",
    "    df = dataframe[dataframe.eq(sensor_or_station).any(axis=1)]\n",
    "    df.reset_index(inplace=True)\n",
    "    # print(df)\n",
    "    if df.shape[0] == 1:\n",
    "        target = df.iloc[0]\n",
    "    elif df.shape[0] > 1:\n",
    "        target = None\n",
    "        for indx, row in df.iterrows():\n",
    "            cond = utc(row.start) <= time <= utc(row.end)\n",
    "            if cond:\n",
    "                target = row\n",
    "    else:\n",
    "        print('There is not proper data in the network dataframe.')\n",
    "        target = None\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveforms(pick, root):\n",
    "    station_details = get_true_station_name(\n",
    "        dataframe=network_details,\n",
    "        sensor_or_station=pick.waveform_id.station_code,\n",
    "        time=pick.time\n",
    "        )\n",
    "    # stationsr = reversing_dictionary(stations)\n",
    "    # station_code = stationsr.get(pick.waveform_id.station_code)\n",
    "    if station_details is None:\n",
    "        print(f'Station Not Found.\\n{pick.waveform_id.station_code}\\n')\n",
    "        with open('reading-data-problem.txt', 'a') as f:\n",
    "            f.write(f'Station Not Found.\\n{pick.waveform_id.station_code}\\n')\n",
    "        return Stream()\n",
    "    time = pick.time.strftime('%Y%m%d_%H')\n",
    "    st = Stream()\n",
    "    for channel in ['e', 'n', 'z']:\n",
    "        path_data = f'{root}/{station_details.sensor}-*/gcf/{time}00{channel}.gcf'\n",
    "        try:\n",
    "            st += read(path_data)\n",
    "        except Exception as error:\n",
    "            print(f'Skip Data.\\n{error}\\n{path_data}\\n')\n",
    "            with open('reading-data-problem.txt', 'a') as f:\n",
    "                f.write(f'Skip Data.\\n{error}\\n{path_data}\\n')\n",
    "    # st_trim = st.slice(starttime=pick.time-time_before,\n",
    "    #                    endtime=pick.time+time_after)\n",
    "    for tr in st:\n",
    "        tr.stats.network = station_details.network\n",
    "        tr.stats.station = station_details.station\n",
    "        tr.stats.location = station_details.location\n",
    "        tr.stats.channel = station_details.channel + tr.stats.channel[-1]\n",
    "\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be38a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_data(st, pick, before, after):\n",
    "    st.trim(starttime=pick.time-before,\n",
    "            endtime=pick.time+after,\n",
    "            pad=True,\n",
    "            nearest_sample=True,\n",
    "            fill_value=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c45894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(st):\n",
    "    st.merge(-1)\n",
    "    st.detrend('constant')\n",
    "    st.merge(fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45494c",
   "metadata": {
    "id": "be45494c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Writing to SeisBench format\n",
    "\n",
    "Now, we can combine all the above functions together to write a dataset in SeisBench format. For this, we first need to define the path. For this example, we are using the current working directory. A dataset consists of 2 components:\n",
    " - a metadata file, always called `metadata.csv`, which contains all the associated properties of the waveform examples (e.g. trace parameters, source parameters etc.).\n",
    " - a waveforms file, always called `waveforms.hdf5`, containing the raw waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0d5f8",
   "metadata": {
    "id": "30d0d5f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_path = Path(cfg.path.out)\n",
    "metadata_path = base_path / \"metadata.csv\"\n",
    "waveforms_path = base_path / \"waveforms.hdf5\"\n",
    "print(metadata_path, waveforms_path, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69c41a",
   "metadata": {
    "id": "ea69c41a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To write the dataset, we use the `WaveformDataWriter` provided by SeisBench. The writer should always be used as a context manager, i.e., using the `with` statement, as shown below. This is to ensure files are properly clsoed after writing and teardown and cleanup operations are always called when exiting the context manager.\n",
    "\n",
    "First, we need to set the data format for our dataset. We do this by assigning a dictionary to the `writer.data_format` group.\n",
    "\n",
    "Next, we iterate over all event and all picks in the events. Using the functions above, we generate the event and trace metadata and download the waveforms. We then convert the waveforms to a numpy array using the function `stream_to_array` provided in `seisbench.util`.\n",
    "\n",
    "As a last step, we hand the event metadata and the waveforms as numpy array over to the writer using `add_trace`. The writer then automatically takes care of writing out the data in the correct format. It also takes care of performance optimisations that we outline in the further considerations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba60659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_picks(picks, station_name):\n",
    "    picks = [pick for pick in picks\n",
    "             if pick.waveform_id.station_code==station_name]\n",
    "    picks = sorted(picks,\n",
    "                   key= lambda p: p.time)\n",
    "    return picks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533508ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picks_time_difference(picks):\n",
    "    picks_time = [pick.time for pick in picks]\n",
    "    picks_time = sorted(picks_time)\n",
    "    picks_difftime = [time-picks_time[0] for time in picks_time]\n",
    "    return picks_difftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_equal_sps(stream):\n",
    "    sps = stream[0].stats.sampling_rate\n",
    "    assert all(tr.stats.sampling_rate == sps for tr in stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = 60\n",
    "after = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec34f6",
   "metadata": {
    "id": "dbec34f6",
    "outputId": "d50fa6fd-ed8a-4932-c9db-c1fad4246dd1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over events and picks, write to SeisBench format\n",
    "with sbd.WaveformDataWriter(metadata_path, waveforms_path) as writer:\n",
    "\n",
    "    # Define data format\n",
    "    writer.data_format = {\n",
    "        \"dimension_order\": \"CW\",\n",
    "        \"component_order\": \"ZNE\",\n",
    "        \"measurement\": \"velocity\",\n",
    "        \"unit\": \"counts\",\n",
    "        \"instrument_response\": \"not restituted\",\n",
    "    }\n",
    "    n_all = len(catalog)\n",
    "    for index, event in enumerate(catalog):\n",
    "        # if index < 1050:\n",
    "        #     continue\n",
    "        if index % 100 == 0:\n",
    "            print(f'{index} of {n_all} ({index/n_all*100:.2f}%)')\n",
    "        # if index == 10:\n",
    "        #     break\n",
    "\n",
    "        event_params = get_event_params(event)\n",
    "        stations_in_event = {pick.waveform_id.station_code for pick in event.picks}\n",
    "        for station_name in stations_in_event:\n",
    "            picks = select_picks(picks=event.picks,\n",
    "                                 station_name=station_name)\n",
    "            ###\n",
    "            time_diff = get_picks_time_difference(picks)\n",
    "            if max(time_diff) >= 60:\n",
    "                print(f'losing pick, maximume is: {max(time_diff)}')\n",
    "            ###\n",
    "            pick = picks[0]\n",
    "            trace_params = get_trace_params(pick)\n",
    "            waveforms = get_waveforms(pick, cfg.path.stream)\n",
    "            ### Preprocessing waveform\n",
    "            # rename_stream_mapper(waveforms, networks, stations, locations, channels)\n",
    "            preprocessing_data(st=waveforms)\n",
    "            # random = np.random.uniform(-before/2, before/2)\n",
    "            trim_data(waveforms, pick, before=before, after=after)\n",
    "            ### Check remaining data\n",
    "            if len(waveforms) == 0:\n",
    "                # No waveform data available\n",
    "                continue\n",
    "            ###\n",
    "            sampling_rate = waveforms[0].stats.sampling_rate\n",
    "            # Check that the traces have the same sampling rate\n",
    "            checking_equal_sps(stream=waveforms)\n",
    "\n",
    "            actual_t_start, data, _ = sbu.stream_to_array(\n",
    "                waveforms,\n",
    "                component_order=writer.data_format[\"component_order\"],\n",
    "            )\n",
    "            trace_params[\"trace_sampling_rate_hz\"] = sampling_rate\n",
    "            trace_params[\"trace_start_time\"] = str(actual_t_start)\n",
    "\n",
    "            for pick in picks:\n",
    "                sample = (pick.time - actual_t_start) * sampling_rate\n",
    "                trace_params[f\"trace_{pick.phase_hint}_arrival_sample\"] = int(sample)\n",
    "                trace_params[f\"trace_{pick.phase_hint}_status\"] = pick.evaluation_mode\n",
    "\n",
    "            writer.add_trace({**event_params, **trace_params}, data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
