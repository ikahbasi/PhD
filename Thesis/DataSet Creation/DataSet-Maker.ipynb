{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec893556",
   "metadata": {},
   "source": [
    "# Importing Important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynbname\n",
    "import logging\n",
    "import pkg_resources\n",
    "import seisbench.data as sbd\n",
    "import seisbench.util as sbu\n",
    "\n",
    "from pathlib import Path\n",
    "from obspy import read_events\n",
    "from obspy import read\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d76abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:37.138977Z",
     "iopub.status.busy": "2024-10-04T13:38:37.138977Z",
     "iopub.status.idle": "2024-10-04T13:38:37.142755Z",
     "shell.execute_reply": "2024-10-04T13:38:37.142755Z",
     "shell.execute_reply.started": "2024-10-04T13:38:37.138977Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3573be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_path = [r'C:\\Users\\ikahbasi\\OneDrive\\Applications\\GitHub\\SeisRoutine',\n",
    "            r'C:\\Users\\ikahb\\OneDrive\\Applications\\GitHub\\SeisRoutine']\n",
    "for path in lib_path:\n",
    "    sys.path.append(path)\n",
    "##########################################################################\n",
    "import SeisRoutine.catalog as src\n",
    "import SeisRoutine.waveform as srw\n",
    "import SeisRoutine.config as srconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+\n",
    "src = reload(src)\n",
    "srw = reload(srw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d44d86",
   "metadata": {},
   "source": [
    "# Define Some Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b42809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_filename_and_path_of_the_running_code():\n",
    "    \"\"\"\n",
    "    Get the filename and directory path of the currently executing code.\n",
    "    \n",
    "    This function works for both regular Python scripts (.py files) and Jupyter Notebooks\n",
    "    (.ipynb files). For notebooks, it handles both VS Code's environment and standard\n",
    "    Jupyter environments.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (directory_path, filename) of the running code.\n",
    "        \n",
    "    Note:\n",
    "        In Jupyter Notebook environments, returns the notebook name and path.\n",
    "        In regular Python scripts, returns the script name and path.\n",
    "    \"\"\"\n",
    "    _file = sys.argv[0]\n",
    "    name = os.path.basename(_file)\n",
    "    path = os.path.dirname(_file)\n",
    "    if name == \"ipykernel_launcher.py\":\n",
    "        try:\n",
    "            _file = globals()['__vsc_ipynb_file__']\n",
    "            name = os.path.basename(_file)\n",
    "            path = os.path.dirname(_file)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            name = ipynbname.name()\n",
    "            path = ipynbname.path()\n",
    "    return path, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, root, pattern_path):\n",
    "        self.root = root\n",
    "        self.pattern_path = pattern_path\n",
    "        self.stream = None\n",
    "        self.stats  = None\n",
    "\n",
    "    def read(self, time):\n",
    "        pattern = self.pattern_path.format(time=time)\n",
    "        path = f'{self.root}/{pattern}'\n",
    "        logging.info(f'Reading Data: {path}')\n",
    "        # print('Reading Data:', path)\n",
    "        self.stream = read(path)\n",
    "        self.preprocessing_data()\n",
    "        self.stations = list({tr.stats.station for tr in self.stream})\n",
    "\n",
    "    def get_data_related_to_pick(self, pick):\n",
    "        if self.stream is None:\n",
    "            self.read(time=pick.time)\n",
    "        if not pick.waveform_id.station_code in self.stations:\n",
    "            self.read(time=pick.time)\n",
    "        if not pick.time.julday == self.stream[0].stats.starttime.julday:\n",
    "            self.read(time=pick.time)\n",
    "        target_stream = self.stream.select(station=pick.waveform_id.station_code)\n",
    "        return target_stream\n",
    "    \n",
    "    def preprocessing_data(self):\n",
    "        self.sps_check()\n",
    "        self.stream.merge(-1)\n",
    "        self.stream.detrend('constant')\n",
    "        self.stream.merge()\n",
    "        # self.stream.merge(method=1, fill_value=0)\n",
    "        # self.stream.filter('bandpass', freqmin=0.5, freqmax=49, zerophase=True)\n",
    "    \n",
    "    def sps_check(self, sps=100):\n",
    "        print('Available sps:', {tr.stats.sampling_rate for tr in self.stream})\n",
    "        assert all(tr.stats.sampling_rate==sps for tr in self.stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18becd85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:48.155901Z",
     "iopub.status.busy": "2024-10-04T13:41:48.155901Z",
     "iopub.status.idle": "2024-10-04T13:41:48.163187Z",
     "shell.execute_reply": "2024-10-04T13:41:48.162184Z",
     "shell.execute_reply.started": "2024-10-04T13:41:48.155901Z"
    },
    "id": "18becd85",
    "outputId": "80b15fee-195d-468a-fa9f-e80e368c21fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_event_params(event):\n",
    "    origin = event.preferred_origin()\n",
    "    mag = event.preferred_magnitude()\n",
    "\n",
    "    source_id = str(event.resource_id)\n",
    "\n",
    "    event_params = {\n",
    "        \"source_id\": source_id,\n",
    "        \"source_origin_time\": str(origin.time),\n",
    "        \"source_origin_uncertainty_sec\": origin.time_errors[\"uncertainty\"],\n",
    "        \"source_latitude_deg\": origin.latitude,\n",
    "        \"source_latitude_uncertainty_km\": origin.latitude_errors[\"uncertainty\"],\n",
    "        \"source_longitude_deg\": origin.longitude,\n",
    "        \"source_longitude_uncertainty_km\": origin.longitude_errors[\"uncertainty\"],\n",
    "        \"source_depth_km\": origin.depth / 1e3            if origin.depth else None,\n",
    "        \"source_depth_uncertainty_km\": origin.depth_errors[\"uncertainty\"] / 1e3           if origin.depth else None,\n",
    "    }\n",
    "    if mag is not None:\n",
    "        event_params[\"source_magnitude\"] = mag.mag\n",
    "        event_params[\"source_magnitude_uncertainty\"] = mag.mag_errors[\"uncertainty\"]\n",
    "        event_params[\"source_magnitude_type\"] = mag.magnitude_type\n",
    "        event_params[\"source_magnitude_author\"] = mag.creation_info.agency_id\n",
    "        event_params[\"split\"] = None\n",
    "    return event_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e47c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:52.523593Z",
     "iopub.status.busy": "2024-10-04T13:41:52.523593Z",
     "iopub.status.idle": "2024-10-04T13:41:52.529192Z",
     "shell.execute_reply": "2024-10-04T13:41:52.528189Z",
     "shell.execute_reply.started": "2024-10-04T13:41:52.523593Z"
    },
    "id": "ef0e47c0",
    "outputId": "cf98a9a5-eed4-4fca-ce2f-ff7cb4f9509d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trace_params(pick):\n",
    "    net = pick.waveform_id.network_code\n",
    "    sta = pick.waveform_id.station_code\n",
    "    trace_params = {\n",
    "        \"station_network_code\": net,\n",
    "        \"station_code\": sta,\n",
    "        \"trace_channel\": pick.waveform_id.channel_code[:2],\n",
    "        \"station_location_code\": pick.waveform_id.location_code,\n",
    "        \"evaluation_mode\": pick.evaluation_mode}\n",
    "    return trace_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b959b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:55.235593Z",
     "iopub.status.busy": "2024-10-04T13:41:55.235593Z",
     "iopub.status.idle": "2024-10-04T13:41:55.241642Z",
     "shell.execute_reply": "2024-10-04T13:41:55.240647Z",
     "shell.execute_reply.started": "2024-10-04T13:41:55.235593Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_phase_params(pick, event):\n",
    "    origin = event.preferred_origin()\n",
    "    arrival = src.select_arrival_related_to_the_pick(pick=pick, arrivals=origin.arrivals)\n",
    "    if arrival:\n",
    "        phase_params = arrival.__dict__.copy()\n",
    "        for key in ['resource_id', 'pick_id', 'phase']:\n",
    "            phase_params.pop(key)\n",
    "        phase_params = {f'{key}_{pick.phase_hint}':val for key,val in phase_params.items()}\n",
    "    else:\n",
    "        phase_params = {}\n",
    "    return phase_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533508ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:12.795049Z",
     "iopub.status.busy": "2024-10-04T13:42:12.794050Z",
     "iopub.status.idle": "2024-10-04T13:42:12.800043Z",
     "shell.execute_reply": "2024-10-04T13:42:12.799044Z",
     "shell.execute_reply.started": "2024-10-04T13:42:12.795049Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_picks_time_difference(picks):\n",
    "    picks_time = [pick.time for pick in picks]\n",
    "    picks_time = sorted(picks_time)\n",
    "    picks_difftime = [time-picks_time[0] for time in picks_time]\n",
    "    return picks_difftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2a356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:00.883104Z",
     "iopub.status.busy": "2024-10-04T13:42:00.883104Z",
     "iopub.status.idle": "2024-10-04T13:42:00.888455Z",
     "shell.execute_reply": "2024-10-04T13:42:00.887452Z",
     "shell.execute_reply.started": "2024-10-04T13:42:00.883104Z"
    }
   },
   "outputs": [],
   "source": [
    "def reversing_dictionary(dictionary):\n",
    "    return {v:k for k, v in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd71433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_labeling(stream, dl_pickers):\n",
    "    outputs = {'P': {}, 'S': {}}\n",
    "    for name, picker in dl_pickers.items():\n",
    "        output = picker.classify(stream)\n",
    "        picks = output.picks\n",
    "        creator = output.creator\n",
    "        for pick in picks:\n",
    "            outputs[pick.phase][name] = pick.peak_time\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c51f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe88ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ztest(array, threshold=3):\n",
    "    '''\n",
    "    The outlier detector with Z-score\n",
    "    '''\n",
    "    data_skewness = skew(array)\n",
    "    ###\n",
    "    z_scores = zscore(array)\n",
    "    outliers = np.abs(z_scores) > threshold\n",
    "    # print(f'Number of outliers: {sum(outliers)}')\n",
    "    logging.info(f'Number of outliers: {sum(outliers)}')\n",
    "    return array[~outliers]\n",
    "\n",
    "def window_checking(array, window_len=0.2, min_num_picks=3):\n",
    "    array.sort()\n",
    "    s = np.diff(array).sum()\n",
    "    # print(s)\n",
    "    logging.info(f'{s}')\n",
    "    if (s <= window_len) and (min_num_picks<=array.size):\n",
    "        cond = True\n",
    "    else:\n",
    "        cond = False\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(array, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Interquartile Range\n",
    "    Tukey Fences are robust methods in detecting outliers.\n",
    "    As per the Turkey method, the outliers are the points lying\n",
    "    beyond the upper boundary of Q3 +1.5*IQR and the lower boundary\n",
    "    of Q1 - 1.5*IQR. These boundaries are referred to as outlier fences.\n",
    "    Any data beyond these fences are considered to be outliers.\n",
    "\n",
    "    for some nonnegative constant k. John Tukey proposed this test,\n",
    "    where k = 1.5 indicates an \"outlier\", and k = 3 indicates data\n",
    "    that is \"far out\".\n",
    "    (Fig.1 of the Kristekova_etal_GJI_2021.pdf)\n",
    "\n",
    "    :type values: numpy.ndarray\n",
    "    :param values: one-dimensional number arrays.\n",
    "    :type multiplier: float\n",
    "    :param multiplier: ???\n",
    "\n",
    "    :returns:\n",
    "    :type outliers: numpy.ndarray\n",
    "    :param outliers: A boolean array concerning the size of input `values`.\n",
    "    :type lower: float\n",
    "    :param lower: the lower boundary of the fences (or Q1 - 1.5*IQR).\n",
    "    :type upper: float\n",
    "    :param upper: the upper boundary of the fences (or Q3 +1.5*IQR).\n",
    "    \"\"\"\n",
    "    values_sorted = sorted(array)\n",
    "    midpoint = int(round(len(values_sorted) / 2.0))\n",
    "    q1 = median(values_sorted[:midpoint])\n",
    "    q3 = median(values_sorted[midpoint:])\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - (iqr * multiplier)\n",
    "    upper = q3 + (iqr * multiplier)\n",
    "    inliers_index = (lower <= array) & (array <= upper)\n",
    "    outliers_index = ~inliers_index\n",
    "    inliers = array[inliers_index]\n",
    "    return inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_for_dwa(array):\n",
    "    '''\n",
    "    The formula 4 in the https://doi.org/10.1093/gji/ggae049 article.\n",
    "    '''\n",
    "    if len(array.shape) == 1:\n",
    "        array = np.expand_dims(array, axis=0)\n",
    "    weights = 1 / np.abs(array-array.T).sum(axis=1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def dwa(array):\n",
    "    '''\n",
    "    The formula 3 in the https://doi.org/10.1093/gji/ggae049 article.\n",
    "    '''\n",
    "    weights = weights_for_dwa(array)\n",
    "    weighted_mean = sum(weights*array) / sum(weights)\n",
    "    return weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf674c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimum_pick_time(times, outlier_detector='Z-score'):\n",
    "    if outlier_detector=='Z-score':\n",
    "        times_inlier = ztest(array=times, threshold=1)\n",
    "    elif outlier_detector=='IQR':\n",
    "        times_inlier = iqr(array=times, multiplier=0.5)\n",
    "    # print(times_inlier)\n",
    "    cond = window_checking(array=times_inlier,\n",
    "                           window_len=1,\n",
    "                           min_num_picks=2)\n",
    "    # print(cond)\n",
    "    logging.info(f'{cond}')\n",
    "    if cond:\n",
    "        time_optimum = dwa(array=times_inlier)\n",
    "    else:\n",
    "        time_optimum = np.nan\n",
    "    # print(times_inlier)\n",
    "    return time_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9f649",
   "metadata": {},
   "source": [
    "# Initializing the init file and starting logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de598521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:37.627029Z",
     "iopub.status.busy": "2024-10-04T13:38:37.627029Z",
     "iopub.status.idle": "2024-10-04T13:38:37.634843Z",
     "shell.execute_reply": "2024-10-04T13:38:37.634843Z",
     "shell.execute_reply.started": "2024-10-04T13:38:37.627029Z"
    }
   },
   "outputs": [],
   "source": [
    "init_cfg = srconf.load_config('0-init-cfg.yml')\n",
    "cfg = srconf.load_config(\n",
    "    os.path.join(init_cfg.target_config_filepath,\n",
    "                 init_cfg.target_config_filename)\n",
    ")\n",
    "#\n",
    "today_str = datetime.today().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "cfg.mk_dataset.path.outputs.dataset = cfg.mk_dataset.path.outputs.dataset.format(datetime_str=today_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca696c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "srconf.configure_logging(level=cfg.log.level,\n",
    "                         log_format=cfg.log.format,\n",
    "                         mode=cfg.log.mode, colored_console=True,\n",
    "                         filepath=cfg.mk_dataset.path.outputs.dataset,\n",
    "                         filename_prefix=cfg.log.filename_prefix,\n",
    "                         filename=cfg.mk_dataset.path.outputs.log.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d30c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_separator = \"+\" * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path, nb_name = getting_filename_and_path_of_the_running_code()\n",
    "msg = (f\"Logging has started for notebook: {nb_name}.\\n\"\n",
    "       f\"This file is located at: {nb_path}\\n\"\n",
    "       )\n",
    "logging.info(msg)\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d33aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all installed packages and their versions\n",
    "imported_modules = {name.split('.')[0] for name in globals() if name in sys.modules}\n",
    "installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "msg = \"Packages List:\\n\"\n",
    "for package in sorted(installed_packages.keys()):\n",
    "    # if package in imported_modules:\n",
    "    version = installed_packages[package]\n",
    "    msg += f\"{package}=={version}\\n\"\n",
    "logging.info(msg)\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = cfg.__str__()\n",
    "logging.info(f'Configuration File:\\n{msg}')\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ffffa",
   "metadata": {},
   "source": [
    "# Loading Seismic Catalog and network details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:39.955243Z",
     "iopub.status.busy": "2024-10-04T13:38:39.955243Z",
     "iopub.status.idle": "2024-10-04T13:39:06.964043Z",
     "shell.execute_reply": "2024-10-04T13:39:06.962072Z",
     "shell.execute_reply.started": "2024-10-04T13:38:39.955243Z"
    }
   },
   "outputs": [],
   "source": [
    "catalog = read_events(cfg.mk_dataset.path.inputs.catalog)\n",
    "catalog = [ev for ev in catalog if ev.picks != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087f36c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:38.100767Z",
     "iopub.status.busy": "2024-10-04T13:38:38.100767Z",
     "iopub.status.idle": "2024-10-04T13:38:38.130498Z",
     "shell.execute_reply": "2024-10-04T13:38:38.130498Z",
     "shell.execute_reply.started": "2024-10-04T13:38:38.100767Z"
    }
   },
   "outputs": [],
   "source": [
    "network_details = pd.read_csv(cfg.mk_dataset.path.inputs.network_details, dtype=str)\n",
    "network_details.fillna(value='', inplace=True)\n",
    "stations_list = network_details['station'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'SDHR': {}, 'JIGH': {}}\n",
    "for ev in catalog:\n",
    "    for pick in ev.picks:\n",
    "        if pick.waveform_id.station_code in ('SDHR', 'JIGH'):\n",
    "            if pick.time.julday in d[pick.waveform_id.station_code].keys():\n",
    "                d[pick.waveform_id.station_code][pick.time.julday] += 1\n",
    "            else:\n",
    "                d[pick.waveform_id.station_code][pick.time.julday] = 1\n",
    "            # d[pick.waveform_id.station_code].append(pick.time.julday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4784ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "otime = [ev.preferred_origin().time.timestamp for ev in catalog]\n",
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(otime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aef51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src.print_phase_frequency(catalog, case_sensitivity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaef76",
   "metadata": {
    "id": "29aaef76",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the event parameters\n",
    "\n",
    "From the catalog, we extract the event parameters and store them into a dictionary. Here, we only extract a few basic parameters on the source and its magnitude - if available. In addition, we define the split of the dataset into training/development/test partitions. We visualize one example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b7f36",
   "metadata": {
    "id": "917b7f36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the trace parameters\n",
    "\n",
    "From each pick, we extract parameters about the trace and store them in a dictionary. Again, we only extract very basic parameters. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff77b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:54.260033Z",
     "iopub.status.busy": "2024-10-04T13:41:54.260033Z",
     "iopub.status.idle": "2024-10-04T13:41:54.264863Z",
     "shell.execute_reply": "2024-10-04T13:41:54.263860Z",
     "shell.execute_reply.started": "2024-10-04T13:41:54.260033Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for ev in catalog:\n",
    "    origin = ev.preferred_origin()\n",
    "    for pick in ev.picks:\n",
    "        arrival = src.select_arrival_related_to_the_pick(pick=pick, arrivals=origin.arrivals)\n",
    "        if arrival==False:\n",
    "            i += 1\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "        # print(arrival, pick, '\\n\\n', sep='\\n')\n",
    "print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45494c",
   "metadata": {
    "id": "be45494c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Writing to SeisBench format\n",
    "\n",
    "Now, we can combine all the above functions together to write a dataset in SeisBench format. For this, we first need to define the path. For this example, we are using the current working directory. A dataset consists of 2 components:\n",
    " - a metadata file, always called `metadata.csv`, which contains all the associated properties of the waveform examples (e.g. trace parameters, source parameters etc.).\n",
    " - a waveforms file, always called `waveforms.hdf5`, containing the raw waveforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69c41a",
   "metadata": {
    "id": "ea69c41a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To write the dataset, we use the `WaveformDataWriter` provided by SeisBench. The writer should always be used as a context manager, i.e., using the `with` statement, as shown below. This is to ensure files are properly clsoed after writing and teardown and cleanup operations are always called when exiting the context manager.\n",
    "\n",
    "First, we need to set the data format for our dataset. We do this by assigning a dictionary to the `writer.data_format` group.\n",
    "\n",
    "Next, we iterate over all event and all picks in the events. Using the functions above, we generate the event and trace metadata and download the waveforms. We then convert the waveforms to a numpy array using the function `stream_to_array` provided in `seisbench.util`.\n",
    "\n",
    "As a last step, we hand the event metadata and the waveforms as numpy array over to the writer using `add_trace`. The writer then automatically takes care of writing out the data in the correct format. It also takes care of performance optimisations that we outline in the further considerations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4236107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:14.778681Z",
     "iopub.status.busy": "2024-10-04T13:42:14.778681Z",
     "iopub.status.idle": "2024-10-04T13:42:14.782480Z",
     "shell.execute_reply": "2024-10-04T13:42:14.782480Z",
     "shell.execute_reply.started": "2024-10-04T13:42:14.778681Z"
    }
   },
   "outputs": [],
   "source": [
    "get_waveforms = get_data(cfg.mk_dataset.path.inputs.stream_root,\n",
    "                         cfg.mk_dataset.path.inputs.stream_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e33355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sbm.EQTransformer()\n",
    "# for n in model.list_pretrained():\n",
    "#     print(n)\n",
    "#     try:\n",
    "#         model.from_pretrained(n)\n",
    "#     except Exception as error:\n",
    "#         print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00664cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seisbench.models as sbm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.mk_dataset.autolabeling:\n",
    "    dl_pickers = {'PhaseNet_stead': sbm.PhaseNet.from_pretrained(\"stead\"),\n",
    "                  'PhaseNet_original': sbm.PhaseNet.from_pretrained(\"original\"),\n",
    "                #   'PhaseNet_scedc': sbm.PhaseNet.from_pretrained(\"scedc\"),\n",
    "                #   'PhaseNet_instance': sbm.PhaseNet.from_pretrained(\"instance\"),\n",
    "                  #\n",
    "                  'EQTransformer_stead': sbm.EQTransformer.from_pretrained(\"stead\"),\n",
    "                #   'EQTransformer_original': sbm.EQTransformer.from_pretrained(\"original\"),\n",
    "                #   'EQTransformer_scedc': sbm.EQTransformer.from_pretrained(\"scedc\"),\n",
    "                #   'EQTransformer_instance': sbm.EQTransformer.from_pretrained(\"instance\"),\n",
    "                  #\n",
    "                #   sbm.GPD.from_pretrained(\"stead\"),\n",
    "                  'GPD_original': sbm.GPD.from_pretrained(\"original\"),\n",
    "                #   'GPD_scedc': sbm.GPD.from_pretrained(\"scedc\"),\n",
    "                #   'GPD_instance': sbm.GPD.from_pretrained(\"instance\"),\n",
    "    }\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for key, dl_picker in dl_pickers.items():\n",
    "            dl_picker.cuda();\n",
    "            logging.info(f\"{key} Running on GPU\")\n",
    "    else:\n",
    "        logging.info(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0d5f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:08.525018Z",
     "iopub.status.busy": "2024-10-04T13:42:08.524019Z",
     "iopub.status.idle": "2024-10-04T13:42:08.531011Z",
     "shell.execute_reply": "2024-10-04T13:42:08.530012Z",
     "shell.execute_reply.started": "2024-10-04T13:42:08.525018Z"
    },
    "id": "30d0d5f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_path = Path(cfg.mk_dataset.path.outputs.dataset)\n",
    "metadata_path = base_path / \"metadata.csv\"\n",
    "waveforms_path = base_path / \"waveforms.hdf5\"\n",
    "###\n",
    "if cfg.mk_dataset.save_streams:\n",
    "    stream_path = base_path / \"mseed\"; os.makedirs(stream_path, exist_ok=True)\n",
    "print(metadata_path, waveforms_path, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec34f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:15.395424Z",
     "iopub.status.busy": "2024-10-04T13:42:15.395424Z",
     "iopub.status.idle": "2024-10-04T13:54:30.381554Z",
     "shell.execute_reply": "2024-10-04T13:54:30.380553Z",
     "shell.execute_reply.started": "2024-10-04T13:42:15.395424Z"
    },
    "id": "dbec34f6",
    "outputId": "d50fa6fd-ed8a-4932-c9db-c1fad4246dd1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over events and picks, write to SeisBench format\n",
    "with sbd.WaveformDataWriter(metadata_path, waveforms_path) as writer:\n",
    "\n",
    "    # Define data format\n",
    "    writer.data_format = {\n",
    "        \"dimension_order\": \"CW\",\n",
    "        \"component_order\": \"ZNE\",\n",
    "        \"measurement\": \"velocity\",\n",
    "        \"unit\": \"counts\",\n",
    "        \"instrument_response\": \"not restituted\",\n",
    "    }\n",
    "    n_all = len(catalog)\n",
    "    for index, event in enumerate(catalog):\n",
    "        # if index < 2000:\n",
    "        #     continue\n",
    "        if index % 500 == 0:\n",
    "            logging.info(f'{index} of {n_all} ({index/n_all*100:.2f}%)')\n",
    "        event_params = get_event_params(event)\n",
    "        stations_in_event = {pick.waveform_id.station_code for pick in event.picks}\n",
    "        stations_in_event = {station for station in stations_in_event if station in stations_list}\n",
    "        for station_name in stations_in_event:\n",
    "            picks = src.select_picks(picks=event.picks,\n",
    "                                 station_name=station_name)\n",
    "            if picks == []:\n",
    "                continue\n",
    "            ###\n",
    "            phase_params = {}\n",
    "            for pick in picks:\n",
    "                param = get_phase_params(pick, event)\n",
    "                phase_params.update(param)\n",
    "            ###\n",
    "            time_diff = get_picks_time_difference(picks)\n",
    "            if max(time_diff) >= 60:\n",
    "                logging.warning(f'losing pick, maximume is: {max(time_diff)}')\n",
    "            ###\n",
    "            pick = picks[0]\n",
    "            trace_params = get_trace_params(pick)\n",
    "            waveforms = get_waveforms.get_data_related_to_pick(pick=pick)\n",
    "            waveforms = waveforms.slice(\n",
    "                starttime=pick.time-cfg.mk_dataset.cut_time.before,\n",
    "                endtime=pick.time+cfg.mk_dataset.cut_time.after,\n",
    "                nearest_sample=True\n",
    "                )\n",
    "            ### Check remaining data\n",
    "            if len(waveforms) == 0:\n",
    "                # No waveform data available\n",
    "                # print('There is No WaveForms After Slicing!!!')\n",
    "                logging.warning(f'There is No WaveForms After Slicing!!! [station: {station_name}]')\n",
    "                continue\n",
    "            ###\n",
    "            if (cfg.mk_dataset.noisepad.before!=0) or (cfg.mk_dataset.noisepad.after!=0):\n",
    "                waveforms = srw.st_noise_padding(\n",
    "                    st=waveforms,\n",
    "                    stime=cfg.mk_dataset.noisepad.before,\n",
    "                    etime=cfg.mk_dataset.noisepad.after,\n",
    "                    std_windows=(cfg.mk_dataset.noisepad.std_start, cfg.mk_dataset.noisepad.std_end)\n",
    "                    )\n",
    "            ###\n",
    "            # Check that the traces have the same sampling rate\n",
    "            srw.waveform.uni_sps(st=waveforms, )\n",
    "            sampling_rate = waveforms[0].stats.sampling_rate\n",
    "            number_of_samples = waveforms[0].data.size\n",
    "            actual_t_start, data, _ = sbu.stream_to_array(\n",
    "                waveforms,\n",
    "                component_order=writer.data_format[\"component_order\"],\n",
    "            )\n",
    "            #\n",
    "            trace_params[f\"trace_sample_number\"] = number_of_samples\n",
    "            trace_params[\"trace_sampling_rate_hz\"] = sampling_rate\n",
    "            trace_params[\"trace_start_time\"] = str(actual_t_start)\n",
    "            ###\n",
    "            gaps = [0, 0, 0]\n",
    "            for gap_detail in waveforms.get_gaps():\n",
    "                if gap_detail[3].endswith('Z'):\n",
    "                    gaps[0] += gap_detail[-1]\n",
    "                elif gap_detail[3].endswith('N'):\n",
    "                    gaps[1] += gap_detail[-1]\n",
    "                elif gap_detail[3].endswith('E'):\n",
    "                    gaps[2] += gap_detail[-1]\n",
    "            trace_params[f\"trace_gaps\"] = gaps\n",
    "            ###\n",
    "            trace_params[\"trace_Q1\"] = [np.percentile(_data, 25) for _data in data]\n",
    "            trace_params[\"trace_Q3\"] = [np.percentile(_data, 75) for _data in data]\n",
    "            ###\n",
    "\n",
    "            for pick in picks:\n",
    "                sample = (pick.time - actual_t_start) * sampling_rate\n",
    "                sample = round(sample)\n",
    "                phase_params[f\"trace_{pick.evaluation_mode}_{pick.phase_hint}_arrival_sample\"] = sample\n",
    "                trace_params[f\"trace_{pick.phase_hint}_snr\"] = srw.health_check.routine.compute_snr(\n",
    "                    data=data, pick_idx=sample, noise_window=100, signal_window=200, axis=1, domain='time')\n",
    "                trace_params[f\"trace_{pick.phase_hint}_snr-dB\"] = 20 * np.log10(trace_params[f\"trace_{pick.phase_hint}_snr\"])\n",
    "            # trace_params[f\"trace_mean\"] = np.mean(data, axis=1)\n",
    "            trace_params[f\"trace_rms\"] = np.sqrt(np.mean(np.power(data, 2), axis=1))\n",
    "            trace_params[f\"trace_median\"] = np.median(data, axis=1)\n",
    "            trace_params[f\"trace_max\"] = np.max(data, axis=1)\n",
    "            trace_params[f\"trace_min\"] = np.min(data, axis=1)\n",
    "            ###\n",
    "            if cfg.mk_dataset.save_streams:\n",
    "                otime = event_params['source_origin_time'].replace('-', '').replace(':', '')[:-5]\n",
    "                waveforms.write(stream_path/f'{index}_{otime}_{station_name}.msd', format='MSEED')\n",
    "            ### Auto Labeling\n",
    "            if cfg.mk_dataset.autolabeling:\n",
    "                stime = min([tr.stats.starttime for tr in waveforms])\n",
    "                etime = max([tr.stats.endtime for tr in waveforms])\n",
    "                waveforms.taper(0.2)\n",
    "                ######################################################################\n",
    "                waveforms.trim(\n",
    "                    starttime=stime-(60-cfg.mk_dataset.cut_time.before),\n",
    "                    endtime=etime+(60-cfg.mk_dataset.cut_time.after),\n",
    "                    pad=True, fill_value=0)\n",
    "                ##\n",
    "                # waveforms_padded = srw.st_noise_padding(\n",
    "                #     st=waveforms,\n",
    "                #     stime=60-cfg.cut_time.before,\n",
    "                #     etime=60-cfg.cut_time.after,\n",
    "                #     std_windows=(cfg.noisepad.std_start, cfg.noisepad.std_end))\n",
    "                waveforms_padded = waveforms\n",
    "                ######################################################################\n",
    "                auto_label = auto_labeling(stream=waveforms_padded, dl_pickers=dl_pickers)\n",
    "                for phase_hint, auto_picks in auto_label.items():\n",
    "                    for picker_dataset_name, picker_time in auto_picks.items():\n",
    "                        # print(phase_hint, picker_dataset_name, picker_time)\n",
    "                        sample = (picker_time - actual_t_start) * sampling_rate\n",
    "                        phase_params[f\"trace_autoDL_{picker_dataset_name}_{phase_hint}_arrival_sample\"] = int(sample)\n",
    "            ####\n",
    "            writer.add_trace({**event_params, **trace_params, **phase_params}, data)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_label = auto_labeling(stream=waveforms, dl_pickers=dl_pickers)\n",
    "for phase_hint, auto_picks in auto_label.items():\n",
    "    for picker_dataset_name, picker_time in auto_picks.items():\n",
    "        logging.info(f'{phase_hint}\\n{picker_dataset_name}\\n{picker_time}')\n",
    "        # print(phase_hint, picker_dataset_name, picker_time)\n",
    "        sample = (picker_time - actual_t_start) * sampling_rate\n",
    "        trace_params[f\"trace_{phase_hint}_arrival_sample_autoDL_{picker_dataset_name}\"] = int(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2930fb",
   "metadata": {
    "id": "1c2930fb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Considerations for converting datasets\n",
    "\n",
    "As outlined above, this tutorial provides a very minimal example on converting a dataset. Here we outline additional consideration that should be taken into account when preparing a dataset.\n",
    "\n",
    "- **Grouping picks**: In this example, we created one trace for each pick. Naturally, traces will overlap if multiple picks, e.g., P and S phases, are available for an event at a station. For an example implementation of this grouping operation, have a look [here](https://github.com/seisbench/seisbench/blob/df94dcd86ce66d6a2ee2bd00da3857259fe579bd/seisbench/data/ethz.py#L109) and in the subsequent lines.\n",
    "- **Adding station information**: In this example, we added no station information except its name. In practice, it will often be helpful for users to incorporate, for example, the location of the station. We skipped this step here, because it requires loading station inventories through FDSN. For an example implementation, have a look [here](https://github.com/seisbench/seisbench/blob/df94dcd86ce66d6a2ee2bd00da3857259fe579bd/seisbench/data/ethz.py#L315).\n",
    "- **Memory requirements**: Internally, the `WaveformDataWriter` writes out the the waveforms continuously in blocks (see point below), but keeps all metadata in memory until the dataset is complete. For very large datasets (or very detailed metadata) this can result in several gigabytes of memory consumption. If you are writing such datasets, make sure the available memory on your machine is sufficient.\n",
    "- **Waveform blocks**: Instead of writing each waveform separately, waveforms are written out in blocks. This massively improves IO performance. Have a look at [the documentation](https://seisbench.readthedocs.io/en/stable/pages/data_format.html#traces-blocks) for details on the strategy. We expect that in nearly all cases using the default setting will be a good choice.\n",
    "- **FDSN considerations**: When converting very large datasets, the performance might be limited by the performance of the FDSN webservice. Unfortunately, downloading lots of short waveforms (as required for many machine learning applications) does not seem to be the most favorable use case for FDSN. This leads to rather slow performance when naively downloading the waveforms as outlined above. Instead, it is often helpful to issue [bulk requests](https://docs.obspy.org/master/packages/autogen/obspy.clients.fdsn.client.Client.get_waveforms_bulk.html). In addition, it might be a good choice to first download the waveforms and cache them locally, for example, in .mseed format, and then convert them to SeisBench.\n",
    "\n",
    "For further details on the data format, check out [the data format specification in the SeisBench documentation](https://seisbench.readthedocs.io/en/stable/pages/data_format.html#traces-blocks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6061ceb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "seisbench_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
