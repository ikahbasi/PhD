{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec893556",
   "metadata": {},
   "source": [
    "# Importing Important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynbname\n",
    "import logging\n",
    "import pkg_resources\n",
    "import seisbench.data as sbd\n",
    "import seisbench.util as sbu\n",
    "\n",
    "from pathlib import Path\n",
    "from obspy import read_events\n",
    "from obspy import read\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d76abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:37.138977Z",
     "iopub.status.busy": "2024-10-04T13:38:37.138977Z",
     "iopub.status.idle": "2024-10-04T13:38:37.142755Z",
     "shell.execute_reply": "2024-10-04T13:38:37.142755Z",
     "shell.execute_reply.started": "2024-10-04T13:38:37.138977Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3573be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_path = [r'C:\\Users\\ikahbasi\\OneDrive\\Applications\\GitHub\\SeisRoutine',\n",
    "            r'C:\\Users\\ikahb\\OneDrive\\Applications\\GitHub\\SeisRoutine']\n",
    "for path in lib_path:\n",
    "    sys.path.append(path)\n",
    "##########################################################################\n",
    "import SeisRoutine.catalog as src\n",
    "import SeisRoutine.waveform as srw\n",
    "import SeisRoutine.config as srconf\n",
    "import SeisRoutine.statistics as srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+\n",
    "src = reload(src)\n",
    "srw = reload(srw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d44d86",
   "metadata": {},
   "source": [
    "# Define Some Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b42809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_filename_and_path_of_the_running_code():\n",
    "    \"\"\"\n",
    "    Get the filename and directory path of the currently executing code.\n",
    "    \n",
    "    This function works for both regular Python scripts (.py files) and Jupyter Notebooks\n",
    "    (.ipynb files). For notebooks, it handles both VS Code's environment and standard\n",
    "    Jupyter environments.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (directory_path, filename) of the running code.\n",
    "        \n",
    "    Note:\n",
    "        In Jupyter Notebook environments, returns the notebook name and path.\n",
    "        In regular Python scripts, returns the script name and path.\n",
    "    \"\"\"\n",
    "    _file = sys.argv[0]\n",
    "    name = os.path.basename(_file)\n",
    "    path = os.path.dirname(_file)\n",
    "    if name == \"ipykernel_launcher.py\":\n",
    "        try:\n",
    "            _file = globals()['__vsc_ipynb_file__']\n",
    "            name = os.path.basename(_file)\n",
    "            path = os.path.dirname(_file)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            name = ipynbname.name()\n",
    "            path = ipynbname.path()\n",
    "    return path, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_data:\n",
    "    def __init__(self, root, pattern_path):\n",
    "        self.root = root\n",
    "        self.pattern_path = pattern_path\n",
    "        self.stream = None\n",
    "        self.stats  = None\n",
    "\n",
    "    def read(self, time):\n",
    "        pattern = self.pattern_path.format(time=time)\n",
    "        path = f'{self.root}/{pattern}'\n",
    "        logging.info(f'Reading Data: {path}')\n",
    "        self.stream = read(path)\n",
    "        self.preprocessing_data()\n",
    "        self.stations = list({tr.stats.station for tr in self.stream})\n",
    "\n",
    "    def get_data_related_to_pick(self, pick):\n",
    "        if self.stream is None:\n",
    "            self.read(time=pick.time)\n",
    "        if not pick.waveform_id.station_code in self.stations:\n",
    "            self.read(time=pick.time)\n",
    "        if not pick.time.julday == self.stream[0].stats.starttime.julday:\n",
    "            self.read(time=pick.time)\n",
    "        target_stream = self.stream.select(station=pick.waveform_id.station_code)\n",
    "        return target_stream\n",
    "    \n",
    "    def preprocessing_data(self):\n",
    "        self.sps_check()\n",
    "        self.stream.merge(-1)\n",
    "        self.stream.detrend('constant')\n",
    "        self.stream.merge()\n",
    "        # self.stream.merge(method=1, fill_value=0)\n",
    "        # self.stream.filter('bandpass', freqmin=0.5, freqmax=49, zerophase=True)\n",
    "    \n",
    "    def sps_check(self, sps=100):\n",
    "        # print('Available sps:', {tr.stats.sampling_rate for tr in self.stream})\n",
    "        assert all(tr.stats.sampling_rate==sps for tr in self.stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18becd85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:48.155901Z",
     "iopub.status.busy": "2024-10-04T13:41:48.155901Z",
     "iopub.status.idle": "2024-10-04T13:41:48.163187Z",
     "shell.execute_reply": "2024-10-04T13:41:48.162184Z",
     "shell.execute_reply.started": "2024-10-04T13:41:48.155901Z"
    },
    "id": "18becd85",
    "outputId": "80b15fee-195d-468a-fa9f-e80e368c21fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_source_params(event):\n",
    "    origin = event.preferred_origin()\n",
    "    mag = event.preferred_magnitude()\n",
    "\n",
    "    source_id = str(event.resource_id)\n",
    "\n",
    "    event_params = {\n",
    "        \"source_id\": source_id,\n",
    "        \"source_origin_time\": str(origin.time),\n",
    "        \"source_origin_uncertainty_sec\": origin.time_errors[\"uncertainty\"],\n",
    "        \"source_latitude_deg\": origin.latitude,\n",
    "        \"source_latitude_uncertainty_km\": origin.latitude_errors[\"uncertainty\"],\n",
    "        \"source_longitude_deg\": origin.longitude,\n",
    "        \"source_longitude_uncertainty_km\": origin.longitude_errors[\"uncertainty\"],\n",
    "        \"source_depth_km\": origin.depth,\n",
    "        \"source_depth_uncertainty_km\": origin.depth_errors[\"uncertainty\"],\n",
    "    }\n",
    "    ### Unit conversion\n",
    "    if event_params['source_depth_km']:\n",
    "        event_params['source_depth_km'] /= 1e3\n",
    "    if event_params['source_depth_uncertainty_km']:\n",
    "        event_params['source_depth_uncertainty_km'] /= 1e3\n",
    "    if event_params['source_latitude_uncertainty_km']:\n",
    "        event_params['source_latitude_uncertainty_km'] *= 111\n",
    "    if event_params['source_longitude_uncertainty_km']:\n",
    "        event_params['source_latitude_uncertainty_km'] *= 111\n",
    "\n",
    "    if mag is not None:\n",
    "        event_params[\"source_magnitude\"] = mag.mag\n",
    "        event_params[\"source_magnitude_uncertainty\"] = mag.mag_errors[\"uncertainty\"]\n",
    "        event_params[\"source_magnitude_type\"] = mag.magnitude_type\n",
    "        event_params[\"source_magnitude_author\"] = mag.creation_info.agency_id\n",
    "        event_params[\"split\"] = None\n",
    "    return event_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7393e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_station_parameters_of_network_details(df):\n",
    "    network = {}\n",
    "    for index, row in df.iterrows():\n",
    "        network[row.station] = {\n",
    "            'station_code': row.station,\n",
    "            'station_network_code': row.network,\n",
    "            'station_location_code': row.location,\n",
    "            'station_latitude_deg': row.latitude,\n",
    "            'station_longitude_deg': row.longitude,\n",
    "            'station_elevation_m': row.elevation,\n",
    "            'station_sensitivity_counts_spm': None,\n",
    "            'station_sensor': row.sensor,\n",
    "            'station_region': row.region,\n",
    "            }\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e47c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:52.523593Z",
     "iopub.status.busy": "2024-10-04T13:41:52.523593Z",
     "iopub.status.idle": "2024-10-04T13:41:52.529192Z",
     "shell.execute_reply": "2024-10-04T13:41:52.528189Z",
     "shell.execute_reply.started": "2024-10-04T13:41:52.523593Z"
    },
    "id": "ef0e47c0",
    "outputId": "cf98a9a5-eed4-4fca-ce2f-ff7cb4f9509d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trace_params(pick):\n",
    "    net = pick.waveform_id.network_code\n",
    "    sta = pick.waveform_id.station_code\n",
    "    trace_params = {\n",
    "        \"station_network_code\": net,\n",
    "        \"station_code\": sta,\n",
    "        \"trace_channel\": pick.waveform_id.channel_code[:2],\n",
    "        \"station_location_code\": pick.waveform_id.location_code,\n",
    "        \"evaluation_mode\": pick.evaluation_mode}\n",
    "    return trace_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b959b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:55.235593Z",
     "iopub.status.busy": "2024-10-04T13:41:55.235593Z",
     "iopub.status.idle": "2024-10-04T13:41:55.241642Z",
     "shell.execute_reply": "2024-10-04T13:41:55.240647Z",
     "shell.execute_reply.started": "2024-10-04T13:41:55.235593Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_phase_params(pick, event):\n",
    "    origin = event.preferred_origin()\n",
    "    arrival = src.select_arrival_related_to_the_pick(pick=pick,\n",
    "                                                     arrivals=origin.arrivals)\n",
    "    if arrival:\n",
    "        phase_params = arrival.__dict__.copy()\n",
    "        for key in ['resource_id', 'pick_id', 'phase', 'takeoff_angle_errors',\n",
    "                    'horizontal_slowness_residual',\n",
    "                    'horizontal_slowness_weight']:\n",
    "            phase_params.pop(key)\n",
    "        phase_params = {f'{key}_{pick.phase_hint}': val\n",
    "                        for key, val in phase_params.items()}\n",
    "    else:\n",
    "        phase_params = {}\n",
    "    return phase_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c51f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc04ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_to_array_ikahbasi(stream, component_order):\n",
    "    \"\"\"\n",
    "    Converts stream of single station waveforms into a numpy array according to a given component order.\n",
    "    If trace start and end times disagree between component traces, remaining parts are filled with zeros.\n",
    "    Also returns completeness, i.e., the fraction of samples in the output that actually contain data.\n",
    "    Assumes all traces to have the same sampling rate.\n",
    "\n",
    "    :param stream: Stream to convert\n",
    "    :type stream: obspy.Stream\n",
    "    :param component_order: Component order\n",
    "    :type component_order: str\n",
    "    :return: starttime, data, completeness\n",
    "    :rtype: UTCDateTime, np.ndarray, float\n",
    "    \"\"\"\n",
    "    starttime = min(trace.stats.starttime for trace in stream)\n",
    "    endtime = max(trace.stats.endtime for trace in stream)\n",
    "    sampling_rate = stream[0].stats.sampling_rate\n",
    "\n",
    "    samples = int((endtime - starttime) * sampling_rate) + 1\n",
    "\n",
    "    completeness = 0.0\n",
    "    data = np.zeros((len(component_order), samples), dtype=\"float64\")\n",
    "    for c_idx, c in enumerate(component_order):\n",
    "        c_stream = stream.select(channel=f\"??{c}\")\n",
    "        gap_found = False\n",
    "        if c_stream.get_gaps():\n",
    "            gap_found = True\n",
    "        completeness = 0.0\n",
    "        for trace in c_stream:\n",
    "            if not gap_found:\n",
    "                tr_data = trace.data\n",
    "            else:\n",
    "                tr_data = trace.data.data\n",
    "            start_sample = int((trace.stats.starttime - starttime) * sampling_rate)\n",
    "            l = min(len(tr_data), samples - start_sample)\n",
    "            data[c_idx, start_sample : start_sample + l] = tr_data[:l]\n",
    "    nans = np.isnan(data)\n",
    "    completeness = np.sum(~np.isnan(data)) / data.size\n",
    "    return starttime, data, completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9f649",
   "metadata": {},
   "source": [
    "# Initializing the init file and starting logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de598521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:37.627029Z",
     "iopub.status.busy": "2024-10-04T13:38:37.627029Z",
     "iopub.status.idle": "2024-10-04T13:38:37.634843Z",
     "shell.execute_reply": "2024-10-04T13:38:37.634843Z",
     "shell.execute_reply.started": "2024-10-04T13:38:37.627029Z"
    }
   },
   "outputs": [],
   "source": [
    "init_cfg = srconf.load_config('0-init-cfg.yml')\n",
    "cfg_path = os.path.join(init_cfg.target_config_filepath,\n",
    "                        init_cfg.target_config_filename)\n",
    "cfg = srconf.load_config(cfg_path)\n",
    "#\n",
    "today_str = datetime.today().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "cfg.mk_dataset.path.outputs.dataset = cfg.mk_dataset.path.outputs.dataset.format(datetime_str=today_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = os.path.join(init_cfg.target_config_filepath,\n",
    "                        init_cfg.target_config_filename)\n",
    "cfg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124672f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg_path.replace('cfg', 'last_run-cfg'), 'w') as file:\n",
    "    cfg.to_yaml(stream=file, default_flow_style=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca696c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "srconf.configure_logging(level=cfg.log.level,\n",
    "                         log_format=cfg.log.format,\n",
    "                         mode=cfg.log.mode, colored_console=True,\n",
    "                         filepath=cfg.mk_dataset.path.outputs.dataset,\n",
    "                         filename_prefix=cfg.log.filename_prefix,\n",
    "                         filename=cfg.mk_dataset.path.outputs.log.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d30c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_separator = \"+\" * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path, nb_name = getting_filename_and_path_of_the_running_code()\n",
    "msg = (f\"Logging has started for notebook: {nb_name}.\\n\"\n",
    "       f\"This file is located at: {nb_path}\\n\"\n",
    "       )\n",
    "logging.info(msg)\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d33aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all installed packages and their versions\n",
    "imported_modules = {name.split('.')[0] for name in globals() if name in sys.modules}\n",
    "installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "msg = \"Packages List:\\n\"\n",
    "for package in sorted(installed_packages.keys()):\n",
    "    # if package in imported_modules:\n",
    "    version = installed_packages[package]\n",
    "    msg += f\"{package}=={version}\\n\"\n",
    "logging.info(msg)\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = cfg.__str__()\n",
    "logging.info(f'Configuration File:\\n{msg}')\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ffffa",
   "metadata": {},
   "source": [
    "# Loading Seismic Catalog and network details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:39.955243Z",
     "iopub.status.busy": "2024-10-04T13:38:39.955243Z",
     "iopub.status.idle": "2024-10-04T13:39:06.964043Z",
     "shell.execute_reply": "2024-10-04T13:39:06.962072Z",
     "shell.execute_reply.started": "2024-10-04T13:38:39.955243Z"
    }
   },
   "outputs": [],
   "source": [
    "catalog = read_events(cfg.mk_dataset.path.inputs.catalog)\n",
    "catalog = [ev for ev in catalog if ev.picks != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087f36c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:38:38.100767Z",
     "iopub.status.busy": "2024-10-04T13:38:38.100767Z",
     "iopub.status.idle": "2024-10-04T13:38:38.130498Z",
     "shell.execute_reply": "2024-10-04T13:38:38.130498Z",
     "shell.execute_reply.started": "2024-10-04T13:38:38.100767Z"
    }
   },
   "outputs": [],
   "source": [
    "network_details = pd.read_csv(cfg.mk_dataset.path.inputs.network_details, dtype=str)\n",
    "network_details.fillna(value='', inplace=True)\n",
    "network_parameters = make_station_parameters_of_network_details(df=network_details)\n",
    "stations_network_list = network_parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just for Ahar\n",
    "d = {'SDHR': {}, 'JIGH': {}}\n",
    "for ev in catalog:\n",
    "    for pick in ev.picks:\n",
    "        if pick.waveform_id.station_code in ('SDHR', 'JIGH'):\n",
    "            if pick.time.julday in d[pick.waveform_id.station_code].keys():\n",
    "                d[pick.waveform_id.station_code][pick.time.julday] += 1\n",
    "            else:\n",
    "                d[pick.waveform_id.station_code][pick.time.julday] = 1\n",
    "            # d[pick.waveform_id.station_code].append(pick.time.julday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4784ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# otime = [ev.preferred_origin().time.timestamp for ev in catalog]\n",
    "# import matplotlib.pyplot as plt\n",
    "# _ = plt.hist(otime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aef51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src.print_phase_frequency(catalog, case_sensitivity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaef76",
   "metadata": {
    "id": "29aaef76",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the event parameters\n",
    "\n",
    "From the catalog, we extract the event parameters and store them into a dictionary. Here, we only extract a few basic parameters on the source and its magnitude - if available. In addition, we define the split of the dataset into training/development/test partitions. We visualize one example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b7f36",
   "metadata": {
    "id": "917b7f36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the trace parameters\n",
    "\n",
    "From each pick, we extract parameters about the trace and store them in a dictionary. Again, we only extract very basic parameters. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff77b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:41:54.260033Z",
     "iopub.status.busy": "2024-10-04T13:41:54.260033Z",
     "iopub.status.idle": "2024-10-04T13:41:54.264863Z",
     "shell.execute_reply": "2024-10-04T13:41:54.263860Z",
     "shell.execute_reply.started": "2024-10-04T13:41:54.260033Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_or_dont_have_related_arrival(catalog):\n",
    "    dont = 0\n",
    "    do = 0\n",
    "    for ev in catalog:\n",
    "        origin = ev.preferred_origin()\n",
    "        for pick in ev.picks:\n",
    "            arrival = src.select_arrival_related_to_the_pick(\n",
    "                pick=pick, arrivals=origin.arrivals)\n",
    "            if arrival==False:\n",
    "                dont += 1\n",
    "            else:\n",
    "                do += 1\n",
    "    return do, dont\n",
    "    \n",
    "do, dont = do_or_dont_have_related_arrival(catalog)\n",
    "msg = (\"Pick arrival situation:\\n\"\n",
    "       f\"{do} Picks have related arrivals.\\n\"\n",
    "       f\"{dont} Picks don't have related arrivals.\")\n",
    "logging.info(msg)\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45494c",
   "metadata": {
    "id": "be45494c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Writing to SeisBench format\n",
    "\n",
    "Now, we can combine all the above functions together to write a dataset in SeisBench format. For this, we first need to define the path. For this example, we are using the current working directory. A dataset consists of 2 components:\n",
    " - a metadata file, always called `metadata.csv`, which contains all the associated properties of the waveform examples (e.g. trace parameters, source parameters etc.).\n",
    " - a waveforms file, always called `waveforms.hdf5`, containing the raw waveforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69c41a",
   "metadata": {
    "id": "ea69c41a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To write the dataset, we use the `WaveformDataWriter` provided by SeisBench. The writer should always be used as a context manager, i.e., using the `with` statement, as shown below. This is to ensure files are properly clsoed after writing and teardown and cleanup operations are always called when exiting the context manager.\n",
    "\n",
    "First, we need to set the data format for our dataset. We do this by assigning a dictionary to the `writer.data_format` group.\n",
    "\n",
    "Next, we iterate over all event and all picks in the events. Using the functions above, we generate the event and trace metadata and download the waveforms. We then convert the waveforms to a numpy array using the function `stream_to_array` provided in `seisbench.util`.\n",
    "\n",
    "As a last step, we hand the event metadata and the waveforms as numpy array over to the writer using `add_trace`. The writer then automatically takes care of writing out the data in the correct format. It also takes care of performance optimisations that we outline in the further considerations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4236107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:14.778681Z",
     "iopub.status.busy": "2024-10-04T13:42:14.778681Z",
     "iopub.status.idle": "2024-10-04T13:42:14.782480Z",
     "shell.execute_reply": "2024-10-04T13:42:14.782480Z",
     "shell.execute_reply.started": "2024-10-04T13:42:14.778681Z"
    }
   },
   "outputs": [],
   "source": [
    "get_stream = get_data(cfg.mk_dataset.path.inputs.stream_root,\n",
    "                      cfg.mk_dataset.path.inputs.stream_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0d5f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:08.525018Z",
     "iopub.status.busy": "2024-10-04T13:42:08.524019Z",
     "iopub.status.idle": "2024-10-04T13:42:08.531011Z",
     "shell.execute_reply": "2024-10-04T13:42:08.530012Z",
     "shell.execute_reply.started": "2024-10-04T13:42:08.525018Z"
    },
    "id": "30d0d5f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_path = Path(cfg.mk_dataset.path.outputs.dataset)\n",
    "metadata_path = base_path / \"metadata.csv\"\n",
    "waveforms_path = base_path / \"waveforms.hdf5\"\n",
    "###\n",
    "msg = (\"Dataset will be save at:\\n\"\n",
    "       + str(metadata_path)\n",
    "       + '\\n'\n",
    "       + str(waveforms_path))\n",
    "###\n",
    "if cfg.mk_dataset.save_streams:\n",
    "    stream_path = base_path / \"mseed\"\n",
    "    os.makedirs(stream_path, exist_ok=True)\n",
    "    msg += '\\n'+ str(stream_path)\n",
    "\n",
    "logging.info(msg)\n",
    "logging.info(f\"Separator: {log_separator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec34f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T13:42:15.395424Z",
     "iopub.status.busy": "2024-10-04T13:42:15.395424Z",
     "iopub.status.idle": "2024-10-04T13:54:30.381554Z",
     "shell.execute_reply": "2024-10-04T13:54:30.380553Z",
     "shell.execute_reply.started": "2024-10-04T13:42:15.395424Z"
    },
    "id": "dbec34f6",
    "outputId": "d50fa6fd-ed8a-4932-c9db-c1fad4246dd1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over events and picks, write to SeisBench format\n",
    "n_passed_picks = 0\n",
    "n_all_picks = do\n",
    "n_all_events = len(catalog)\n",
    "n_events_step = 100\n",
    "with sbd.WaveformDataWriter(metadata_path, waveforms_path) as writer:\n",
    "\n",
    "    # Define data format\n",
    "    writer.data_format = {\n",
    "        \"dimension_order\":     cfg.mk_dataset.parameters.dimension_order,\n",
    "        \"component_order\":     cfg.mk_dataset.parameters.component_order,\n",
    "        \"sampling_rate\":       cfg.mk_dataset.parameters.sampling_rate,\n",
    "        \"measurement\":         cfg.mk_dataset.parameters.measurement,\n",
    "        \"unit\":                cfg.mk_dataset.parameters.unit,\n",
    "        \"instrument_response\": cfg.mk_dataset.parameters.instrument_response,\n",
    "    }\n",
    "    for n_passed_events, event in enumerate(catalog):\n",
    "        origin = event.preferred_origin()\n",
    "        ########################################################################\n",
    "        ## Selecting stations to processing\n",
    "        ## Option 1: using all stations that exist in the event catalog.\n",
    "        ##           (variable: stations_event_list)\n",
    "        ## Option 2: using just stations that exist in the network details file.\n",
    "        ##           (variable: stations_network_list)\n",
    "        ########################################################################\n",
    "        stations_event_list = {pick.waveform_id.station_code\n",
    "                               for pick in event.picks}\n",
    "        _stations = stations_network_list\n",
    "        for station_name in _stations:\n",
    "            picks = src.select_picks(picks=event.picks,\n",
    "                                     station_name=station_name)\n",
    "            if picks == []:\n",
    "                continue\n",
    "            ###\n",
    "            pick = picks[0]\n",
    "            stime = pick.time - cfg.mk_dataset.cut_time.before\n",
    "            etime = pick.time + cfg.mk_dataset.cut_time.after\n",
    "            ### Reading Data\n",
    "            st = get_stream.get_data_related_to_pick(pick=pick)\n",
    "            st = st.slice(starttime=stime,\n",
    "                          endtime=etime,\n",
    "                          nearest_sample=True)\n",
    "            # It's possible that all data were masked! If not split,\n",
    "            # N empty traces exist and len(st) shows N.\n",
    "            st = st.split()\n",
    "            ### Check remaining data\n",
    "            if len(st) == 0:\n",
    "                msg = ('There is No WaveForms After Slicing!!!\\n'\n",
    "                       f'Station: {station_name}\\n'\n",
    "                       f'Otime:   {origin.time}\\n'\n",
    "                       f'Pick:    {pick.time}')\n",
    "                logging.warning(msg)\n",
    "                continue\n",
    "            \n",
    "            st.detrend('constant')\n",
    "            st.merge()\n",
    "            ### Check that the traces have the same sampling rate\n",
    "            srw.waveform.uni_sps(st=st, )\n",
    "            # starttime, data, completeness = sbu.stream_to_array(\n",
    "            #     stream=st,\n",
    "            #     component_order=cfg.mk_dataset.parameters.component_order)\n",
    "            starttime, data, completeness = stream_to_array_ikahbasi(\n",
    "                stream=st,\n",
    "                component_order=cfg.mk_dataset.parameters.component_order)\n",
    "\n",
    "            ########################\n",
    "            ### Trace parameters ###\n",
    "            ########################\n",
    "            trace_params = {}\n",
    "            tr = st[0]\n",
    "            sps = tr.stats.sampling_rate\n",
    "            # trace_params['trace_name'] = 'Kaki'\n",
    "            trace_params[\"trace_start_time\"] = str(starttime)\n",
    "            trace_params[\"trace_npts\"] = data.shape[-1]\n",
    "            trace_params[\"trace_sampling_rate_hz\"] = sps\n",
    "            trace_params[\"trace_dt_s\"] = tr.stats.delta\n",
    "            trace_params[\"trace_channel\"] = tr.stats.channel[:2]\n",
    "            trace_params[\"trace_category\"] = \"earthquake\"\n",
    "            trace_params[\"trace_completeness\"] = completeness\n",
    "            component_order = cfg.mk_dataset.parameters.component_order\n",
    "            ###\n",
    "            pick = sorted(picks, key=lambda x: x.time)[0]\n",
    "            sample = (pick.time - starttime) * sps\n",
    "            sample = int(round(sample))\n",
    "            snr = srw.health_check.routine.compute_snr(\n",
    "                data=data, pick_idx=sample,\n",
    "                noise_window=100, signal_window=200, axis=1, domain='time')\n",
    "            epsilon = 1e-10\n",
    "            snr[snr<epsilon] += epsilon\n",
    "            tmp = {'snr': snr,\n",
    "                   'snr_db': 20 * np.log10(snr)}\n",
    "            for key, vals in tmp.items():\n",
    "                for cha, val in zip(component_order, vals):\n",
    "                    trace_params[f\"trace_{cha}_{key}\"] = val\n",
    "            ###\n",
    "            tmp = {'median': np.median(data, axis=1),\n",
    "                   'mean': np.mean(data, axis=1),\n",
    "                   'rms': np.sqrt(np.mean(np.power(data, 2), axis=1)),\n",
    "                   'max': np.max(data, axis=1),\n",
    "                   'min': np.min(data, axis=1),\n",
    "                   'lower_quartile': [np.percentile(_, 25) for _ in data],\n",
    "                   'upper_quartile': [np.percentile(_, 75) for _ in data],\n",
    "                   'gap': np.sum(np.isnan(data), axis=1)}\n",
    "            for key, vals in tmp.items():\n",
    "                for cha, val in zip(component_order, vals):\n",
    "                    trace_params[f\"trace_{cha}_{key}_counts\"] = val\n",
    "            ### Note: I changed the 'trace_*_arrival_sample' keywords.\n",
    "            for pick in picks:\n",
    "                sample = (pick.time - starttime) * sps\n",
    "                sample = int(round(sample))\n",
    "                hint = pick.phase_hint\n",
    "                hint = hint if hint!='' else None\n",
    "                trace_params[f\"trace_{hint}_arrival_sample\"] = sample\n",
    "                trace_params[f\"trace_{hint}_status\"] = pick.evaluation_mode\n",
    "\n",
    "            #########################\n",
    "            ### Source parameters ###\n",
    "            #########################\n",
    "            source_params = get_source_params(event)\n",
    "\n",
    "            ##########################\n",
    "            ### Station parameters ###\n",
    "            ##########################\n",
    "            station_params = network_parameters[station_name]\n",
    "\n",
    "            #######################\n",
    "            ### Path parameters ###\n",
    "            #######################\n",
    "            path_params = {}\n",
    "            \n",
    "            for pick in picks:\n",
    "                arrival = src.select_arrival_related_to_the_pick(\n",
    "                    pick=pick, arrivals=origin.arrivals)\n",
    "                if arrival:\n",
    "                    hint = pick.phase_hint\n",
    "                    hint = hint if hint!='' else None\n",
    "                    path_params[f'path_{hint}_travel_s'] = pick.time - origin.time\n",
    "                    path_params[f'path_{hint}_residual_s'] = arrival.time_residual\n",
    "                    #\n",
    "                    azimuth = arrival.azimuth\n",
    "                    if azimuth:\n",
    "                        if azimuth < 180:\n",
    "                            back_azimuth = azimuth + 180\n",
    "                        elif azimuth >= 180:\n",
    "                            back_azimuth = azimuth + 180\n",
    "                        path_params['path_azimuth_deg'] = azimuth\n",
    "                        path_params['path_back_azimuth_deg'] = back_azimuth\n",
    "                    if arrival.distance:\n",
    "                        path_params['path_ep_distance_km'] = arrival.distance * 111\n",
    "\n",
    "            ############################\n",
    "            ### Write In The DataSet ###\n",
    "            ############################\n",
    "            writer.add_trace(waveform=data,\n",
    "                             metadata={**trace_params,\n",
    "                                       **source_params,\n",
    "                                       **station_params,\n",
    "                                       **path_params})\n",
    "            \n",
    "            #####################\n",
    "            ### Saving stream ###\n",
    "            #####################\n",
    "            if cfg.mk_dataset.save_streams:\n",
    "                otime = origin.time\n",
    "                otime = otime.replace('-', '').replace(':', '')[:-5]\n",
    "                stream_name = f'{n_passed_events}_{otime}_{station_name}.msd'\n",
    "                st.write(stream_path/stream_name, format='MSEED')\n",
    "            n_passed_picks += len(picks)\n",
    "        n_passed_events += 1\n",
    "        ### Write log\n",
    "        if n_passed_events % n_events_step == 0:\n",
    "            msg = (\"Passed Processes:\\n\"\n",
    "                   f\"{n_passed_events} of {n_all_events} events passed \"\n",
    "                   f\"({n_passed_events/n_all_events*100:.2f}%).\\n\"\n",
    "                   f\"{n_passed_picks} of {n_all_picks} picks passed \"\n",
    "                   f\"({n_passed_picks/n_all_picks*100:.2f}%).\")\n",
    "            logging.info(msg)\n",
    "\n",
    "msg = (\"Passed Processes:\\n\"\n",
    "       f\"{n_passed_events} of {n_all_events} events passed \"\n",
    "       f\"({n_passed_events/n_all_events*100:.2f}%).\\n\"\n",
    "       f\"{n_passed_picks} of {n_all_picks} picks passed \"\n",
    "       f\"({n_passed_picks/n_all_picks*100:.2f}%).\")\n",
    "logging.info(msg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "seisbench_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
